{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision einops accelerate diffusers transformers webdataset omegaconf h5py tqdm open_clip_torch sentence-transformers deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64623078",
   "metadata": {},
   "source": [
    "## 1. 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 推論設定 - 必要に応じて変更してください\n",
    "# =============================================================================\n",
    "\n",
    "# 推論モード: \"dummy\" | \"real\"\n",
    "INFERENCE_MODE = \"dummy\"\n",
    "\n",
    "# 被験者ID\n",
    "SUBJECT = \"subj01\"\n",
    "\n",
    "# データパス\n",
    "DATA_ROOT = \"/content/drive/MyDrive/algonauts_2023_challenge_data\"\n",
    "\n",
    "# 学習済みモデルのパス\n",
    "MODEL_CKPT = \"/content/drive/MyDrive/mindeye_checkpoints/algonauts_subj01_dummy/last.pth\"\n",
    "\n",
    "# unCLIP/SDXLモデルのパス（realモード時）\n",
    "UNCLIP_CKPT = None  # 例: \"/content/drive/MyDrive/unclip6_epoch0_step110000.ckpt\"\n",
    "\n",
    "# 出力設定\n",
    "NUM_SAMPLES = 4  # 生成するサンプル数\n",
    "OUTPUT_DIR = \"/content/outputs\"\n",
    "\n",
    "# =============================================================================\n",
    "DUMMY_MODE = (INFERENCE_MODE == \"dummy\")\n",
    "print(f\"Inference Mode: {INFERENCE_MODE}\")\n",
    "print(f\"Subject: {SUBJECT}\")\n",
    "print(f\"Model: {MODEL_CKPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43617c",
   "metadata": {},
   "source": [
    "## 2. 環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ab1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c723ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存パッケージ\n",
    "!pip install -q torch torchvision einops omegaconf h5py tqdm\n",
    "\n",
    "if not DUMMY_MODE:\n",
    "    !pip install -q open_clip_torch diffusers transformers kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リポジトリ準備\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.exists(\"/content/MindEyeV2\"):\n",
    "    !git clone https://github.com/YOUR_USERNAME/MindEyeV2.git /content/MindEyeV2\n",
    "\n",
    "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
    "os.chdir(\"/content/MindEyeV2\")\n",
    "\n",
    "# 環境変数設定\n",
    "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33775386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveマウント\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# 出力ディレクトリ作成\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb5d57",
   "metadata": {},
   "source": [
    "## 3. モデルロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# モジュールインポート\n",
    "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
    "from models_algonauts import AlgonautsMindEye\n",
    "from transfer_utils import load_checkpoint, get_state_dict_from_checkpoint\n",
    "from config import DEVICE\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェックポイントからモデル設定を復元\n",
    "checkpoint = load_checkpoint(MODEL_CKPT)\n",
    "\n",
    "# モデル設定（ckptに保存されていれば使用、なければデフォルト）\n",
    "hidden_dim = checkpoint.get(\"hidden_dim\", 256)\n",
    "subject = checkpoint.get(\"subject\", SUBJECT)\n",
    "\n",
    "print(f\"Checkpoint info:\")\n",
    "print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Subject: {subject}\")\n",
    "print(f\"  Final loss: {checkpoint.get('final_loss', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル作成\n",
    "model = AlgonautsMindEye(\n",
    "    subjects=[subject],\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_len=1,\n",
    "    n_blocks=4,\n",
    "    use_prior=False,\n",
    "    blurry_recon=False,\n",
    ")\n",
    "\n",
    "# 重みロード\n",
    "state_dict = get_state_dict_from_checkpoint(checkpoint)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "print(f\"Total vertices: {get_total_vertices(subject)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a26cd9",
   "metadata": {},
   "source": [
    "## 4. 画像生成器の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DUMMY_MODE:\n",
    "    from dummy_models import DummyDiffusionEngine\n",
    "    \n",
    "    diffusion_engine = DummyDiffusionEngine(output_size=512).to(DEVICE)\n",
    "    print(\"Using DummyDiffusionEngine (outputs will be random)\")\n",
    "    \n",
    "    def generate_image(clip_tokens):\n",
    "        \"\"\"ダミー画像生成\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # ダミー潜在変数\n",
    "            z = torch.randn(clip_tokens.shape[0], 4, 64, 64, device=DEVICE)\n",
    "            images = diffusion_engine.decode_first_stage(z)\n",
    "        return images\n",
    "\n",
    "else:\n",
    "    # 本物のunCLIPを使用\n",
    "    print(\"Loading real diffusion model...\")\n",
    "    \n",
    "    # SGM/unCLIPのロード（要実装）\n",
    "    # この部分は元のrecon_inference.ipynbを参照して実装\n",
    "    raise NotImplementedError(\n",
    "        \"Real inference mode requires additional setup. \"\n",
    "        \"Please implement unCLIP loading based on src/recon_inference.ipynb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e97ca3",
   "metadata": {},
   "source": [
    "## 5. データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット\n",
    "dataset = AlgonautsDataset(\n",
    "    data_root=DATA_ROOT,\n",
    "    subject=subject,\n",
    "    split=\"train\",\n",
    "    load_images=True,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルインデックスをランダムに選択\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(dataset), NUM_SAMPLES, replace=False)\n",
    "print(f\"Sample indices: {sample_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2209e93",
   "metadata": {},
   "source": [
    "## 6. 推論実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"ImageNet正規化を元に戻す\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(tensor.device)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def to_pil(tensor):\n",
    "    \"\"\"Tensor to PIL Image\"\"\"\n",
    "    if tensor.dim() == 4:\n",
    "        tensor = tensor[0]\n",
    "    img = tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論実行\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in sample_indices:\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # fMRI入力\n",
    "        fmri = sample['fmri'].unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # 元画像\n",
    "        original_image = sample['image'].unsqueeze(0)\n",
    "        \n",
    "        # fMRI → CLIP tokens\n",
    "        backbone, clip_voxels, _ = model(fmri)\n",
    "        \n",
    "        # CLIP tokens → 画像\n",
    "        recon_images = generate_image(clip_voxels)\n",
    "        \n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'original': denormalize(original_image),\n",
    "            'reconstruction': recon_images,\n",
    "            'clip_tokens': clip_voxels,\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed sample {idx}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(results)} reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13a8fe",
   "metadata": {},
   "source": [
    "## 7. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を並べて表示\n",
    "fig, axes = plt.subplots(NUM_SAMPLES, 2, figsize=(8, 4 * NUM_SAMPLES))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    # 元画像\n",
    "    ax_orig = axes[i, 0] if NUM_SAMPLES > 1 else axes[0]\n",
    "    orig_img = result['original'][0].cpu().permute(1, 2, 0).numpy()\n",
    "    orig_img = np.clip(orig_img, 0, 1)\n",
    "    ax_orig.imshow(orig_img)\n",
    "    ax_orig.set_title(f\"Original (idx={result['index']})\")\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # 再構成画像\n",
    "    ax_recon = axes[i, 1] if NUM_SAMPLES > 1 else axes[1]\n",
    "    recon_img = result['reconstruction'][0].cpu().permute(1, 2, 0).numpy()\n",
    "    recon_img = np.clip(recon_img, 0, 1)\n",
    "    ax_recon.imshow(recon_img)\n",
    "    ax_recon.set_title(f\"Reconstruction {'(DUMMY)' if DUMMY_MODE else ''}\")\n",
    "    ax_recon.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"reconstruction_comparison.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved to: {os.path.join(OUTPUT_DIR, 'reconstruction_comparison.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dff247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個別画像を保存\n",
    "for i, result in enumerate(results):\n",
    "    # 元画像\n",
    "    orig_pil = to_pil(result['original'])\n",
    "    orig_pil.save(os.path.join(OUTPUT_DIR, f\"original_{result['index']:04d}.png\"))\n",
    "    \n",
    "    # 再構成画像\n",
    "    recon_pil = to_pil(result['reconstruction'])\n",
    "    recon_pil.save(os.path.join(OUTPUT_DIR, f\"recon_{result['index']:04d}.png\"))\n",
    "\n",
    "print(f\"Saved {len(results)} image pairs to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbeca2",
   "metadata": {},
   "source": [
    "## 8. CLIP特徴の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ce926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP特徴の統計\n",
    "all_clips = torch.cat([r['clip_tokens'] for r in results], dim=0)\n",
    "\n",
    "print(f\"CLIP tokens shape: {all_clips.shape}\")\n",
    "print(f\"Mean: {all_clips.mean().item():.4f}\")\n",
    "print(f\"Std: {all_clips.std().item():.4f}\")\n",
    "print(f\"Min: {all_clips.min().item():.4f}\")\n",
    "print(f\"Max: {all_clips.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac17c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP特徴のヒストグラム\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(all_clips.cpu().numpy().flatten(), bins=100, alpha=0.7)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of CLIP Token Values\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e81b2",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "1. **ダミーモードで動作確認** → エラーなく画像が出力されればOK\n",
    "2. **realモードで本格推論** → unCLIPモデルをロードして実行\n",
    "3. **定量評価** → 元画像との類似度を計算\n",
    "4. **transformer_brain_encoder統合** → 画像→fMRI→画像の往復テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MedARC-AI/MindEyeV2.git\n",
    "%cd MindEyeV2/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79085f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "cache_dir = './'\n",
    "# 軽量モデル (hidden_dim=1024, low-level module なし)\n",
    "hf_hub_download('pscotti/mindeyev2', 'train_logs/multisubject_subj01_1024hid_nolow_300ep/last.pth', repo_type='dataset', local_dir=cache_dir)\n",
    "hf_hub_download('pscotti/mindeyev2', 'unclip6_epoch0_step110000.ckpt', repo_type='dataset', local_dir=cache_dir)\n",
    "hf_hub_download('pscotti/mindeyev2', 'bigG_to_L_epoch8.pth', repo_type='dataset', local_dir=cache_dir)\n",
    "print('downloaded model files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493f475",
   "metadata": {},
   "source": [
    "## fMRI ベクトルのアップロード\n",
    "前処理ノートで作成した `voxel.npy` をアップロードしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import numpy as np, torch\n",
    "uploaded = files.upload()\n",
    "voxel = torch.tensor(np.load(list(uploaded.keys())[0])).float()[None,None]\n",
    "print('voxel shape:', voxel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259089a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "import utils\n",
    "from models import BrainNetwork, PriorNetwork, BrainDiffusionPrior\n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision='fp16')\n",
    "device = accelerator.device\n",
    "hidden_dim = 1024  # 軽量モデル用\n",
    "clip_seq_dim, clip_emb_dim = 256, 1664\n",
    "\n",
    "# 事前に ckpt を読み込んでボクセル次元を取得\n",
    "ckpt = torch.load('./train_logs/multisubject_subj01_1024hid_nolow_300ep/last.pth', map_location='cpu', weights_only=False)\n",
    "expected_voxels = ckpt['model_state_dict']['ridge.linears.0.weight'].shape[1]\n",
    "print('expected voxels from ckpt:', expected_voxels)\n",
    "\n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return x\n",
    "\n",
    "class RidgeRegression(nn.Module):\n",
    "    def __init__(self, input_sizes, out_features):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(s, out_features) for s in input_sizes])\n",
    "    def forward(self, x, subj_idx):\n",
    "        return self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "\n",
    "model = MindEyeModule()\n",
    "model.ridge = RidgeRegression([expected_voxels], out_features=hidden_dim)\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, blurry_recon=False)  # low-level なし\n",
    "prior_network = PriorNetwork(dim=clip_emb_dim, depth=6, dim_head=52, heads=clip_emb_dim//52, causal=False, num_tokens=clip_seq_dim, learned_query_mode='pos_emb')\n",
    "model.diffusion_prior = BrainDiffusionPrior(net=prior_network, image_embed_dim=clip_emb_dim, condition_on_text_encodings=False, timesteps=100, cond_drop_prob=0.2, image_embed_scale=None)\n",
    "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "model.to(device).eval().requires_grad_(False)\n",
    "print('model ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('generative_models'))\n",
    "from omegaconf import OmegaConf\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "config = OmegaConf.load('generative_models/configs/unclip6.yaml')\n",
    "params = OmegaConf.to_container(config, resolve=True)['model']['params']\n",
    "params['first_stage_config']['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "params['sampler_config']['params']['num_steps'] = 38\n",
    "diffusion_engine = DiffusionEngine(network_config=params['network_config'], denoiser_config=params['denoiser_config'], first_stage_config=params['first_stage_config'], conditioner_config=params['conditioner_config'], sampler_config=params['sampler_config'], scale_factor=params['scale_factor'], disable_first_stage_autocast=params['disable_first_stage_autocast']).to(device)\n",
    "diffusion_engine.load_state_dict(torch.load('unclip6_epoch0_step110000.ckpt', map_location='cpu')['state_dict'])\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "batch = {'jpg': torch.randn(1,3,1,1).to(device), 'original_size_as_tuple': torch.ones(1,2).to(device)*768, 'crop_coords_top_left': torch.zeros(1,2).to(device)}\n",
    "vector_suffix = diffusion_engine.conditioner(batch)['vector'].to(device)\n",
    "print('unclip ready', vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained('microsoft/git-large-coco')\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained('microsoft/git-large-coco').to(device)\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "class CLIPConverter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, 257)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, 1024)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x.permute(0,2,1))\n",
    "        return self.linear2(x.permute(0,2,1))\n",
    "clip_convert = CLIPConverter().to(device)\n",
    "clip_convert.load_state_dict(torch.load('bigG_to_L_epoch8.pth', map_location='cpu')['model_state_dict'])\n",
    "print('caption model ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# voxel 次元を ckpt に合わせる（不足はゼロ埋め、超過は切り詰め）\n",
    "if voxel.shape[-1] < expected_voxels:\n",
    "    pad = expected_voxels - voxel.shape[-1]\n",
    "    voxel = torch.cat([voxel, torch.zeros(*voxel.shape[:-1], pad)], dim=-1)\n",
    "elif voxel.shape[-1] > expected_voxels:\n",
    "    voxel = voxel[..., :expected_voxels]\n",
    "\n",
    "model.eval(); clip_text_model.eval(); diffusion_engine.eval()\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    voxel_d = voxel.to(device)\n",
    "    voxel_ridge = model.ridge(voxel_d, 0)\n",
    "    backbone, clip_voxels, _ = model.backbone(voxel_ridge)\n",
    "    prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, text_cond={'text_embed': backbone}, cond_scale=1., timesteps=20)\n",
    "    pred_caption_emb = clip_convert(prior_out)\n",
    "    generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    samples = utils.unclip_recon(prior_out, diffusion_engine, vector_suffix, num_samples=1)[0].cpu()\n",
    "print('caption:', caption)\n",
    "plt.imshow(transforms.ToPILImage()(samples)); plt.axis('off'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e04e0d",
   "metadata": {},
   "source": [
    "## 1. 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18df81",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Google Driveをマウント（既にマウント済みの場合はスキップ）\n",
    "\n",
    "# マウント\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# zipファイルのパス（例：Google Drive上のアップロード先）\n",
    "zip_path = \"/content/drive/MyDrive/algonauts_2023_challenge_data.zip\"  # 変更してください\n",
    "\n",
    "# 展開先ディレクトリ\n",
    "extract_path = \"/content/drive/MyDrive/\"  # 変更してください\n",
    "\n",
    "# zipファイルの確認と展開\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"Extracting {zip_path} to {extract_path}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(f\"Zip file not found at: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 学習設定 - 必要に応じて変更してください\n",
    "# =============================================================================\n",
    "\n",
    "# 学習モード: \"dummy\" | \"light\" | \"standard\"\n",
    "TRAIN_MODE = \"dummy\"\n",
    "\n",
    "# 被験者ID\n",
    "SUBJECT = \"subj01\"\n",
    "\n",
    "# データパス（Google Drive上）\n",
    "DATA_ROOT = \"/content/drive/MyDrive/algonauts_2023_challenge_data\"\n",
    "\n",
    "# チェックポイント保存先\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/mindeye_checkpoints\"\n",
    "\n",
    "# 既存の学習済みモデル（転移学習用、Noneの場合はスキップ）\n",
    "PRETRAINED_CKPT = None  # 例: \"/content/drive/MyDrive/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
    "\n",
    "# =============================================================================\n",
    "# モード別設定（自動設定）\n",
    "# =============================================================================\n",
    "if TRAIN_MODE == \"dummy\":\n",
    "    HIDDEN_DIM = 256\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 1\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = True\n",
    "elif TRAIN_MODE == \"light\":\n",
    "    HIDDEN_DIM = 512\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "else:  # standard\n",
    "    HIDDEN_DIM = 1024\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    USE_PRIOR = True\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "\n",
    "print(f\"Mode: {TRAIN_MODE}\")\n",
    "print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Dummy Mode: {DUMMY_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892613",
   "metadata": {},
   "source": [
    "## 2. 環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef90d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a979c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存パッケージのインストール\n",
    "!pip install -q torch torchvision einops omegaconf h5py tqdm accelerate\n",
    "\n",
    "# ダミーモードでない場合は追加パッケージをインストール\n",
    "if not DUMMY_MODE:\n",
    "    !pip install -q open_clip_torch diffusers transformers kornia webdataset dalle2_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リポジトリのクローン（既存の場合はスキップ）\n",
    "import os\n",
    "if not os.path.exists(\"/content/MindEyeV2\"):\n",
    "    # TODO: 自分のフォークしたリポジトリURLに変更してください\n",
    "    !git clone https://github.com/YOUR_USERNAME/MindEyeV2.git /content/MindEyeV2\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "# パスを追加\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
    "\n",
    "os.chdir(\"/content/MindEyeV2\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveをマウント\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# データの存在確認\n",
    "import os\n",
    "subj_dir = os.path.join(DATA_ROOT, SUBJECT)\n",
    "if os.path.exists(subj_dir):\n",
    "    print(f\"✓ Data found: {subj_dir}\")\n",
    "    !ls -la {subj_dir}\n",
    "else:\n",
    "    print(f\"✗ Data NOT found: {subj_dir}\")\n",
    "    print(\"Please upload Algonauts2023 data to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf51bdb",
   "metadata": {},
   "source": [
    "## 3. データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 環境変数でダミーモードを設定\n",
    "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
    "\n",
    "# mysrcモジュールをインポート\n",
    "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
    "from config import print_config, DEVICE\n",
    "\n",
    "print_config()\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Total vertices for {SUBJECT}: {get_total_vertices(SUBJECT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダー作成\n",
    "train_loader = get_dataloader(\n",
    "    data_root=DATA_ROOT,\n",
    "    subject=SUBJECT,\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# サンプルバッチを取得\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  fMRI shape: {sample_batch['fmri'].shape}\")\n",
    "print(f\"  Image shape: {sample_batch['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3882cc",
   "metadata": {},
   "source": [
    "## 4. モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_algonauts import AlgonautsMindEye, create_algonauts_model\n",
    "from transfer_utils import (\n",
    "    load_pretrained_without_ridge,\n",
    "    freeze_layers,\n",
    "    get_trainable_params,\n",
    "    print_parameter_summary,\n",
    ")\n",
    "\n",
    "# モデル作成\n",
    "model = create_algonauts_model(\n",
    "    subjects=[SUBJECT],\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    seq_len=1,\n",
    "    n_blocks=4,\n",
    "    use_prior=USE_PRIOR,\n",
    "    blurry_recon=BLURRY_RECON,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Model created!\")\n",
    "print_parameter_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 転移学習（既存ckptがある場合）\n",
    "if PRETRAINED_CKPT and os.path.exists(PRETRAINED_CKPT):\n",
    "    print(f\"Loading pretrained weights from: {PRETRAINED_CKPT}\")\n",
    "    loaded, missing = load_pretrained_without_ridge(\n",
    "        model,\n",
    "        PRETRAINED_CKPT,\n",
    "        freeze_backbone=True,\n",
    "        freeze_prior=True,\n",
    "    )\n",
    "    print(f\"\\nAfter transfer learning:\")\n",
    "    print_parameter_summary(model)\n",
    "else:\n",
    "    print(\"No pretrained checkpoint specified. Training from scratch.\")\n",
    "    # スクラッチ学習の場合はbackboneもfreezeしない\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d8f04",
   "metadata": {},
   "source": [
    "## 5. CLIP特徴の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミーモードの場合はダミーCLIPを使用\n",
    "if DUMMY_MODE:\n",
    "    from dummy_models import DummyCLIPImageEmbedder, get_dummy_clip_features\n",
    "    \n",
    "    clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)\n",
    "    print(\"Using DummyCLIPImageEmbedder\")\n",
    "else:\n",
    "    # 本物のCLIPを使用\n",
    "    try:\n",
    "        import open_clip\n",
    "        \n",
    "        # ViT-bigG-14 のロード（重い）\n",
    "        print(\"Loading OpenCLIP ViT-bigG-14... (this may take a while)\")\n",
    "        clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            \"ViT-bigG-14\",\n",
    "            pretrained=\"laion2b_s39b_b160k\",\n",
    "        )\n",
    "        clip_model = clip_model.to(DEVICE).eval()\n",
    "        \n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(\"OpenCLIP loaded!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load OpenCLIP: {e}\")\n",
    "        print(\"Falling back to dummy mode\")\n",
    "        DUMMY_MODE = True\n",
    "        from dummy_models import DummyCLIPImageEmbedder\n",
    "        clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_features(images):\n",
    "    \"\"\"画像からCLIP特徴を抽出\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if DUMMY_MODE:\n",
    "            return clip_embedder(images)\n",
    "        else:\n",
    "            # OpenCLIPを使用\n",
    "            features = clip_model.encode_image(images)\n",
    "            return features\n",
    "\n",
    "# テスト\n",
    "test_images = sample_batch['image'].to(DEVICE)\n",
    "test_features = get_clip_features(test_images)\n",
    "print(f\"CLIP features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e701814",
   "metadata": {},
   "source": [
    "## 6. 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d53128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# オプティマイザ（学習可能なパラメータのみ）\n",
    "trainable_params = get_trainable_params(model, mode=\"all_unfrozen\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "optimizer = AdamW(trainable_params, lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "# スケジューラ\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip_loss(preds, targets, temp=0.006):\n",
    "    \"\"\"Soft CLIP contrastive loss\"\"\"\n",
    "    # Flatten to (batch, dim)\n",
    "    preds = preds.view(preds.shape[0], -1)\n",
    "    targets = targets.view(targets.shape[0], -1)\n",
    "    \n",
    "    # Normalize\n",
    "    preds = F.normalize(preds, dim=-1)\n",
    "    targets = F.normalize(targets, dim=-1)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    logits = (preds @ targets.T) / temp\n",
    "    labels = torch.arange(len(logits), device=logits.device)\n",
    "    \n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "def train_step(batch):\n",
    "    \"\"\"1バッチの学習ステップ\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # データ取得\n",
    "    fmri = batch['fmri'].to(DEVICE)\n",
    "    images = batch['image'].to(DEVICE)\n",
    "    \n",
    "    # CLIP特徴を取得（ターゲット）\n",
    "    with torch.no_grad():\n",
    "        clip_target = get_clip_features(images)\n",
    "    \n",
    "    # Forward\n",
    "    backbone, clip_voxels, blurry = model(fmri)\n",
    "    \n",
    "    # Loss計算\n",
    "    # clip_voxels: (batch, seq, emb_dim)\n",
    "    # clip_target: (batch, seq, emb_dim) or (batch, emb_dim)\n",
    "    if clip_target.dim() == 2:\n",
    "        clip_target = clip_target.unsqueeze(1)\n",
    "    \n",
    "    loss = soft_clip_loss(clip_voxels, clip_target)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d188f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ確認\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ループ\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_loader)} batches/epoch\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        loss = train_step(batch)\n",
    "        epoch_losses.append(loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # メモリ確認\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e4d2f",
   "metadata": {},
   "source": [
    "## 7. チェックポイント保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b93a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfer_utils import save_checkpoint\n",
    "\n",
    "# 保存先ディレクトリ作成\n",
    "save_dir = os.path.join(CHECKPOINT_DIR, f\"algonauts_{SUBJECT}_{TRAIN_MODE}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# チェックポイント保存\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=NUM_EPOCHS,\n",
    "    save_path=os.path.join(save_dir, \"last.pth\"),\n",
    "    extra_info={\n",
    "        \"train_mode\": TRAIN_MODE,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"final_loss\": losses[-1] if losses else None,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"\\nCheckpoint saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c46259",
   "metadata": {},
   "source": [
    "## 8. 簡易検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dee4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論テスト\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_fmri = test_batch['fmri'].to(DEVICE)\n",
    "    test_images = test_batch['image'].to(DEVICE)\n",
    "    \n",
    "    # fMRI → CLIP tokens\n",
    "    backbone, clip_voxels, blurry = model(test_fmri)\n",
    "    \n",
    "    # 実際のCLIP特徴\n",
    "    clip_target = get_clip_features(test_images)\n",
    "    if clip_target.dim() == 2:\n",
    "        clip_target = clip_target.unsqueeze(1)\n",
    "    \n",
    "    # コサイン類似度\n",
    "    pred_flat = F.normalize(clip_voxels.view(clip_voxels.shape[0], -1), dim=-1)\n",
    "    target_flat = F.normalize(clip_target.view(clip_target.shape[0], -1), dim=-1)\n",
    "    \n",
    "    similarity = (pred_flat * target_flat).sum(dim=-1).mean()\n",
    "    \n",
    "    print(f\"\\nInference test:\")\n",
    "    print(f\"  Input fMRI shape: {test_fmri.shape}\")\n",
    "    print(f\"  Output CLIP shape: {clip_voxels.shape}\")\n",
    "    print(f\"  Average cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"ImageNet正規化を元に戻す\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# サンプル画像を表示\n",
    "sample_images = denormalize(test_images[:4])\n",
    "grid = make_grid(sample_images, nrow=4).cpu().permute(1, 2, 0).numpy()\n",
    "grid = np.clip(grid, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(grid)\n",
    "plt.title(\"Sample Training Images\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5bbd3",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "1. **ダミーモードで動作確認** → エラーなく完了すればOK\n",
    "2. **軽量モード（light）で実学習** → T4で数時間\n",
    "3. **標準モード（standard）で本格学習** → Pro or 研究室PC\n",
    "4. **推論ノートブック** → `mindeye_inference_colab.ipynb` で画像再構成"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

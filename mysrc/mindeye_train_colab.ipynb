{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e04e0d",
   "metadata": {
    "id": "30e04e0d"
   },
   "source": [
    "## 1. è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18df81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f18df81",
    "outputId": "d9856e11-1c3f-46ac-ff52-1a1f88a6c309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Extracting /content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip to /content/data/algonauts_2023_challenge_data/train_data/\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆï¼ˆæ—¢ã«ãƒã‚¦ãƒ³ãƒˆæ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "\n",
    "# ãƒã‚¦ãƒ³ãƒˆ\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# zipãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹ï¼šGoogle Driveä¸Šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆï¼‰\n",
    "zip_path = \"/content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip\"  # å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "\n",
    "# å±•é–‹å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "extract_path = \"/content/data/algonauts_2023_challenge_data/train_data/\"  # å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "\n",
    "# zipãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨å±•é–‹\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"Extracting {zip_path} to {extract_path}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(f\"Zip file not found at: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de1933e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1de1933e",
    "outputId": "f376733f-5873-4edb-8e4d-0188e90ce00a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: light\n",
      "Hidden Dim: 512\n",
      "Batch Size: 4\n",
      "Epochs: 10\n",
      "Dummy Mode: False\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# å­¦ç¿’è¨­å®š - å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "# =============================================================================\n",
    "\n",
    "# å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: \"dummy\" | \"light\" | \"standard\"\n",
    "TRAIN_MODE = \"light\"\n",
    "\n",
    "# è¢«é¨“è€…ID\n",
    "SUBJECT = \"subj01\"\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveä¸Šï¼‰\n",
    "DATA_ROOT = \"/mount/nfs6/visitor/nsd/algonauts_2023_challenge_data/train_data\"#\"/content/data/algonauts_2023_challenge_data/train_data/\"\n",
    "\n",
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å…ˆ\n",
    "CHECKPOINT_DIR = \"/mount/nfs6/visitor/nsd/folder/mindeye_checkpoints\"#\"/content/data/mindeye_checkpoints\"\n",
    "\n",
    "# æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»¢ç§»å­¦ç¿’ç”¨ã€Noneã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "PRETRAINED_CKPT = None  # ä¾‹: \"/content/drive/MyDrive/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
    "\n",
    "# =============================================================================\n",
    "# ãƒ¢ãƒ¼ãƒ‰åˆ¥è¨­å®šï¼ˆè‡ªå‹•è¨­å®šï¼‰\n",
    "# =============================================================================\n",
    "if TRAIN_MODE == \"dummy\":\n",
    "    HIDDEN_DIM = 256\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 1\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = True\n",
    "elif TRAIN_MODE == \"light\":\n",
    "    HIDDEN_DIM = 512\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "else:  # standard\n",
    "    HIDDEN_DIM = 1024\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    USE_PRIOR = True\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "\n",
    "print(f\"Mode: {TRAIN_MODE}\")\n",
    "print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Dummy Mode: {DUMMY_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892613",
   "metadata": {
    "id": "9f892613"
   },
   "source": [
    "## 2. ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef90d8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fef90d8a",
    "outputId": "07539b64-de84-4e32-e606-fd942ca07579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 13 16:55:43 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             40W /  300W |      14MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             44W /  300W |      14MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2164      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A            2164      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPUã®ç¢ºèª\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a979c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a979c8",
    "outputId": "df42c61f-2e73-4f93-e43f-d79380da228f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "âœ“ DUMMY_MODE: Skipping heavy libraries (dalle2-pytorch, kornia, pytorch_lightning)\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆå¸¸ã«å¿…è¦ï¼‰\n",
    "!pip install -q torch torchvision tqdm einops h5py pillow matplotlib requests numpy\n",
    "!pip install -q omegaconf accelerate\n",
    "\n",
    "# OpenAI CLIPï¼ˆsrc/models.pyãŒå¿…è¦ï¼‰\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# webdatasetï¼ˆsrc/utils.pyãŒå¿…è¦ï¼‰\n",
    "!pip install -q webdataset\n",
    "\n",
    "# OpenCLIP\n",
    "!pip install -q open_clip_torch\n",
    "\n",
    "# âš ï¸ DUMMY_MODE=Trueã®å ´åˆã¯é‡ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—\n",
    "if DUMMY_MODE:\n",
    "    print(\"âœ“ DUMMY_MODE: Skipping heavy libraries (dalle2-pytorch, kornia, pytorch_lightning)\")\n",
    "else:\n",
    "    # æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ç”¨ã®é‡ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "    !pip install -q pytorch_lightning\n",
    "    !pip install -q kornia\n",
    "    !pip install -q dalle2-pytorch\n",
    "    !pip install -q diffusers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062b8c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1062b8c4",
    "outputId": "6e739e11-1a4d-4120-e468-5d884de0c63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists\n",
      "Working directory: /content/MindEyeV2\n"
     ]
    }
   ],
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆæ—¢å­˜ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "import os\n",
    "if not os.path.exists(\"/content/MindEyeV2\"):\n",
    "    # TODO: è‡ªåˆ†ã®ãƒ•ã‚©ãƒ¼ã‚¯ã—ãŸãƒªãƒã‚¸ãƒˆãƒªURLã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "    !git clone https://github.com/boxed-mikann/MindEyeV2.git /content/MindEyeV2\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "# ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "import sys\n",
    "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
    "\n",
    "os.chdir(\"/content/MindEyeV2\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79e285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/pcnl_guest3/workspace/MindEyeV2\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "import sys ,os\n",
    "\n",
    "sys.path.insert(0, \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/home/pcnl_guest3/workspace/MindEyeV2/src\")\n",
    "\n",
    "os.chdir(\"//home/pcnl_guest3/workspace/MindEyeV2\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "StGS0JkILkyW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StGS0JkILkyW",
    "outputId": "a707a38a-26fe-4719-b67a-c71ea12fc9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling latest changes from MindEyeV2 repository...\n",
      "Already up to date.\n",
      "Repository update complete.\n"
     ]
    }
   ],
   "source": [
    "print('Pulling latest changes from MindEyeV2 repository...')\n",
    "!git pull\n",
    "print('Repository update complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6985daca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6985daca",
    "outputId": "8d4288ca-a5ac-42b7-cdd0-173671013d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data found: /mount/nfs6/visitor/nsd/algonauts_2023_challenge_data/train_data/subj01\n",
      "åˆè¨ˆ 20\n",
      "drwxrwxr-x  5 pcnl_guest3 visitor 4096  1æœˆ 11  2023 .\n",
      "drwxrwxr-x 10 shinji      visitor 4096 12æœˆ 16 17:31 ..\n",
      "drwxr-x---  2 pcnl_guest3 visitor 4096 12æœˆ 10  2022 roi_masks\n",
      "drwxr-x---  3 pcnl_guest3 visitor 4096 12æœˆ 10  2022 test_split\n",
      "drwxr-x---  4 pcnl_guest3 visitor 4096 10æœˆ 31  2022 training_split\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª\n",
    "import os\n",
    "subj_dir = os.path.join(DATA_ROOT, SUBJECT)\n",
    "if os.path.exists(subj_dir):\n",
    "    print(f\"âœ“ Data found: {subj_dir}\")\n",
    "    !ls -la {subj_dir}\n",
    "else:\n",
    "    print(f\"âœ— Data NOT found: {subj_dir}\")\n",
    "    print(\"Please upload Algonauts2023 data to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf51bdb",
   "metadata": {
    "id": "2bf51bdb"
   },
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3bafa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f3bafa2",
    "outputId": "026509ff-274c-4047-9800-9e7ead94ffe0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ç’°å¢ƒå¤‰æ•°ã§ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã§ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
    "\n",
    "# mysrcãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
    "from config import print_config, DEVICE\n",
    "\n",
    "print_config()\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Total vertices for {SUBJECT}: {get_total_vertices(SUBJECT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0pp7KTZjWpDh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pp7KTZjWpDh",
    "outputId": "126e792e-9741-4044-8817-e492bcd8b470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š Memory Usage BEFORE Model Creation\n",
      "============================================================\n",
      "  cpu_mb: 557.36 MB\n",
      "  gpu_allocated_mb: 0.00 MB\n",
      "  gpu_reserved_mb: 0.00 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” ãƒ¡ãƒ¢ãƒªãƒ‡ãƒãƒƒã‚° - ãƒ¢ãƒ‡ãƒ«ä½œæˆå‰\n",
    "# =============================================================================\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_info():\n",
    "    \"\"\"CPU/GPUãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¡¨ç¤º\"\"\"\n",
    "    # CPUãƒ¡ãƒ¢ãƒª\n",
    "    process = psutil.Process()\n",
    "    cpu_mem_mb = process.memory_info().rss / 1024**2\n",
    "\n",
    "    # GPUãƒ¡ãƒ¢ãƒª\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_mb = torch.cuda.memory_allocated() / 1024**2\n",
    "        gpu_reserved_mb = torch.cuda.memory_reserved() / 1024**2\n",
    "        return {\n",
    "            \"cpu_mb\": cpu_mem_mb,\n",
    "            \"gpu_allocated_mb\": gpu_mem_mb,\n",
    "            \"gpu_reserved_mb\": gpu_reserved_mb,\n",
    "        }\n",
    "    return {\"cpu_mb\": cpu_mem_mb}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š Memory Usage BEFORE Model Creation\")\n",
    "print(\"=\"*60)\n",
    "mem_before = get_memory_info()\n",
    "for k, v in mem_before.items():\n",
    "    print(f\"  {k}: {v:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936f9ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "936f9ce3",
    "outputId": "3e768d6e-66c0-4469-c7a5-b741162c2176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fMRI data for subj01...\n",
      "  LH shape: (9841, 19004), RH shape: (9841, 20544)\n",
      "  Combined shape: (9841, 39548)\n",
      "Found 9841 train images\n",
      "\n",
      "Dataset size: 9841\n",
      "Number of batches: 2460\n",
      "\n",
      "Sample batch:\n",
      "  fMRI shape: torch.Size([4, 39548])\n",
      "  Image shape: torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\n",
    "train_loader = get_dataloader(\n",
    "    data_root=DATA_ROOT,\n",
    "    subject=SUBJECT,\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒã‚’å–å¾—\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  fMRI shape: {sample_batch['fmri'].shape}\")\n",
    "print(f\"  Image shape: {sample_batch['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3882cc",
   "metadata": {
    "id": "6e3882cc"
   },
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e9968",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce17d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'fmri (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n fmri ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9d2104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f9d2104",
    "outputId": "4d608ab8-fdbf-4194-bc5f-f5a267537fde"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:168\u001b[39m, in \u001b[36mAlgonautsMindEye.__init__\u001b[39m\u001b[34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrainNetwork\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = BrainNetwork(\n\u001b[32m    170\u001b[39m         h=hidden_dim,\n\u001b[32m    171\u001b[39m         in_dim=hidden_dim,  \u001b[38;5;66;03m# Ridgeå‡ºåŠ›ã¨åŒã˜\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m         clip_scale=clip_scale,\n\u001b[32m    179\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/models.py:12\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/utils.py:253\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m txt\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenerative_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msgm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m append_dims\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munclip_recon\u001b[39m(x, diffusion_engine, vector_suffix,\n\u001b[32m    255\u001b[39m                  num_samples=\u001b[32m1\u001b[39m, offset_noise_level=\u001b[32m0.04\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/generative_models/sgm/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoencodingEngine, DiffusionEngine\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_configs_path, instantiate_from_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/generative_models/sgm/models/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoencodingEngine\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiffusion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiffusionEngine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/generative_models/sgm/models/autoencoder.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytorch_lightning'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransfer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     load_pretrained_without_ridge,\n\u001b[32m      4\u001b[39m     freeze_layers,\n\u001b[32m      5\u001b[39m     get_trainable_params,\n\u001b[32m      6\u001b[39m     print_parameter_summary,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ãƒ¢ãƒ‡ãƒ«ä½œæˆ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mcreate_algonauts_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSUBJECT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_PRIOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLURRY_RECON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel created!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m print_parameter_summary(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:304\u001b[39m, in \u001b[36mcreate_algonauts_model\u001b[39m\u001b[34m(subjects, hidden_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, device)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_algonauts_model\u001b[39m(\n\u001b[32m    277\u001b[39m     subjects: List[\u001b[38;5;28mstr\u001b[39m] = [\u001b[33m\"\u001b[39m\u001b[33msubj01\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    278\u001b[39m     hidden_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m1024\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m     device: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    286\u001b[39m ) -> AlgonautsMindEye:\n\u001b[32m    287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    Algonautså¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m \u001b[33;03m        AlgonautsMindEye ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     model = \u001b[43mAlgonautsMindEye\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_emb_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_emb_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_seq_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_seq_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:181\u001b[39m, in \u001b[36mAlgonautsMindEye.__init__\u001b[39m\u001b[34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[39m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = BrainNetwork(\n\u001b[32m    170\u001b[39m         h=hidden_dim,\n\u001b[32m    171\u001b[39m         in_dim=hidden_dim,  \u001b[38;5;66;03m# Ridgeå‡ºåŠ›ã¨åŒã˜\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m         clip_scale=clip_scale,\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import BrainNetwork from src/models.py. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMake sure src/ is in your Python path. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Diffusion Priorï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m.diffusion_prior = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "from models_algonauts import AlgonautsMindEye, create_algonauts_model\n",
    "from transfer_utils import (\n",
    "    load_pretrained_without_ridge,\n",
    "    freeze_layers,\n",
    "    get_trainable_params,\n",
    "    print_parameter_summary,\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "model = create_algonauts_model(\n",
    "    subjects=[SUBJECT],\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    seq_len=1,\n",
    "    n_blocks=4,\n",
    "    use_prior=USE_PRIOR,\n",
    "    blurry_recon=BLURRY_RECON,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Model created!\")\n",
    "print_parameter_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1f86db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb1f86db",
    "outputId": "78153e41-a0f7-4182-8e84-8eb0075839b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained checkpoint specified. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# è»¢ç§»å­¦ç¿’ï¼ˆæ—¢å­˜ckptãŒã‚ã‚‹å ´åˆï¼‰\n",
    "if PRETRAINED_CKPT and os.path.exists(PRETRAINED_CKPT):\n",
    "    print(f\"Loading pretrained weights from: {PRETRAINED_CKPT}\")\n",
    "    loaded, missing = load_pretrained_without_ridge(\n",
    "        model,\n",
    "        PRETRAINED_CKPT,\n",
    "        freeze_backbone=True,\n",
    "        freeze_prior=True,\n",
    "    )\n",
    "    print(f\"\\nAfter transfer learning:\")\n",
    "    print_parameter_summary(model)\n",
    "else:\n",
    "    print(\"No pretrained checkpoint specified. Training from scratch.\")\n",
    "    # ã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’ã®å ´åˆã¯backboneã‚‚freezeã—ãªã„\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3liT9FTmWvmm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3liT9FTmWvmm",
    "outputId": "8cf66267-5178-474a-fa51-d5533e57a4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š Memory Usage AFTER Model Creation\n",
      "============================================================\n",
      "  cpu_mb: 4652.56 MB\n",
      "  gpu_allocated_mb: 490.02 MB\n",
      "  gpu_reserved_mb: 504.00 MB\n",
      "\n",
      "ğŸ“ˆ Memory Increase:\n",
      "  cpu_mb: +4137.39 MB\n",
      "  gpu_allocated_mb: +490.02 MB\n",
      "  gpu_reserved_mb: +504.00 MB\n",
      "\n",
      "ğŸ§® Theoretical Model Size:\n",
      "  Total params: 128,452,504\n",
      "  Trainable params: 128,452,504\n",
      "  FP32 size: 490.01 MB\n",
      "  FP16 size: 245.00 MB\n",
      "\n",
      "ğŸ¯ Model Device: cuda:0\n",
      "   Data Type: torch.float32\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” ãƒ¡ãƒ¢ãƒªãƒ‡ãƒãƒƒã‚° - ãƒ¢ãƒ‡ãƒ«ä½œæˆå¾Œ\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š Memory Usage AFTER Model Creation\")\n",
    "print(\"=\"*60)\n",
    "mem_after = get_memory_info()\n",
    "for k, v in mem_after.items():\n",
    "    print(f\"  {k}: {v:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Memory Increase:\")\n",
    "for k in mem_after.keys():\n",
    "    increase = mem_after[k] - mem_before.get(k, 0)\n",
    "    print(f\"  {k}: +{increase:.2f} MB\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ç†è«–çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆFP32æƒ³å®šï¼‰\n",
    "theoretical_mb = total_params * 4 / 1024**2\n",
    "print(f\"\\nğŸ§® Theoretical Model Size:\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  FP32 size: {theoretical_mb:.2f} MB\")\n",
    "print(f\"  FP16 size: {theoretical_mb/2:.2f} MB\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿéš›ã«GPUä¸Šã«ã‚ã‚‹ã‹ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    sample_param = next(model.parameters())\n",
    "    print(f\"\\nğŸ¯ Model Device: {sample_param.device}\")\n",
    "    print(f\"   Data Type: {sample_param.dtype}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d8f04",
   "metadata": {
    "id": "7b3d8f04"
   },
   "source": [
    "## 5. CLIPç‰¹å¾´ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nAEaVivHYaP-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "nAEaVivHYaP-",
    "outputId": "fef4dd00-43bc-4985-87c2-9dc6ed1eb89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š DummyCLIPImageEmbedder Memory Check\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip_embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-556445613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclip_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclip_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mclip_size_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_params\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;31m# FP32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” CLIP Embedder ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n",
    "# =============================================================================\n",
    "if DUMMY_MODE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“Š DummyCLIPImageEmbedder Memory Check\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\n",
    "    clip_params = sum(p.numel() for p in clip_embedder.parameters())\n",
    "    clip_size_mb = clip_params * 4 / 1024**2  # FP32\n",
    "\n",
    "    print(f\"  Total params: {clip_params:,}\")\n",
    "    print(f\"  Theoretical size (FP32): {clip_size_mb:.2f} MB\")\n",
    "\n",
    "    # ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª\n",
    "    sample_param = next(clip_embedder.parameters())\n",
    "    print(f\"  Device: {sample_param.device}\")\n",
    "    print(f\"  Dtype: {sample_param.dtype}\")\n",
    "\n",
    "    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
    "    mem_info = get_memory_info()\n",
    "    print(f\"\\n  Current CPU RAM: {mem_info['cpu_mb']:.2f} MB\")\n",
    "    if 'gpu_allocated_mb' in mem_info:\n",
    "        print(f\"  Current GPU RAM: {mem_info['gpu_allocated_mb']:.2f} MB\")\n",
    "\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b0f201",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "bc9b5a8cbd3a4119941e9e7b93455500",
      "56d8754cfee447a8a4bbe1c95a6e4ee1",
      "6e5ec983c10741ee87e5fefd371cc748",
      "7996b0610b624a289add344d00a72d9f",
      "650bdbc5a5d744039bf6f7c9aedf59c6",
      "24507560358f462cb8021f8f4ecb42ba",
      "ca7a99b3f5fa4c17b1edadd0f0321314",
      "1369be7e088f48c38298e34d674dd052",
      "2d3f8231f66a4d83b53e90ff2c6a9363",
      "2067b4ce007648cd93ebfa662e873155",
      "37947142af5443ef9a8c764cb956e90c"
     ]
    },
    "id": "f0b0f201",
    "outputId": "4acad06e-62f4-4054-e859-6d159aa77a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenCLIP ViT-B-32 (lightweight version for Colab)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9b5a8cbd3a4119941e9e7b93455500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP loaded! Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼CLIPã‚’ä½¿ç”¨\n",
    "if DUMMY_MODE:\n",
    "    from dummy_models import DummyCLIPImageEmbedder\n",
    "\n",
    "    clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)\n",
    "    print(\"Using DummyCLIPImageEmbedder (è»½é‡ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«)\")\n",
    "\n",
    "else:\n",
    "    # æœ¬ç‰©ã®CLIPã‚’ä½¿ç”¨ï¼ˆè»½é‡ç‰ˆï¼‰\n",
    "    # ViT-bigG-14ã¯5GBä»¥ä¸Šã§Colabç„¡æ–™ç‰ˆã§ã¯ä½¿ãˆãªã„ãŸã‚ã€è»½é‡ç‰ˆã‚’ä½¿ç”¨\n",
    "    try:\n",
    "        import open_clip\n",
    "\n",
    "        # è»½é‡ç‰ˆCLIPï¼ˆViT-B/32: ç´„350MBï¼‰\n",
    "        print(\"Loading OpenCLIP ViT-B-32 (lightweight version for Colab)...\")\n",
    "        clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            \"ViT-B-32\",  # bigG-14ã®ä»£ã‚ã‚Šã«è»½é‡ç‰ˆã‚’ä½¿ç”¨\n",
    "            pretrained=\"laion2b_s34b_b79k\",\n",
    "        )\n",
    "        clip_model = clip_model.to(DEVICE).eval()\n",
    "\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(f\"OpenCLIP loaded! Model on device: {next(clip_model.parameters()).device}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load OpenCLIP: {e}\")\n",
    "        print(\"Falling back to dummy mode\")\n",
    "        DUMMY_MODE = True\n",
    "        from dummy_models import DummyCLIPImageEmbedder\n",
    "        clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412dc816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "412dc816",
    "outputId": "f3460962-0ab6-419f-b759-a4af4063dfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP features shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "def get_clip_features(images):\n",
    "    \"\"\"ç”»åƒã‹ã‚‰CLIPç‰¹å¾´ã‚’æŠ½å‡º\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if DUMMY_MODE:\n",
    "            return clip_embedder(images)\n",
    "        else:\n",
    "            # OpenCLIPã‚’ä½¿ç”¨\n",
    "            features = clip_model.encode_image(images)\n",
    "            return features\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "test_images = sample_batch['image'].to(DEVICE)\n",
    "test_features = get_clip_features(test_images)\n",
    "print(f\"CLIP features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e701814",
   "metadata": {
    "id": "8e701814"
   },
   "source": [
    "## 6. å­¦ç¿’ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6d53128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6d53128",
    "outputId": "2e716f83-9cc4-43b8-e99b-bc321b559e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 128,452,504\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ï¼ˆå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ï¼‰\n",
    "trainable_params = get_trainable_params(model, mode=\"all_unfrozen\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "optimizer = AdamW(trainable_params, lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06b396",
   "metadata": {
    "id": "0d06b396"
   },
   "outputs": [],
   "source": [
    "def soft_clip_loss(preds, targets, temp=0.006):\n",
    "    \"\"\"\n",
    "    Soft CLIP contrastive loss (å…ƒã®utils.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)\n",
    "    \n",
    "    Args:\n",
    "        preds: (batch, emb_dim) - ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ï¼ˆbrain â†’ CLIPï¼‰\n",
    "        targets: (batch, emb_dim) - å®Ÿéš›ã®CLIPç‰¹å¾´\n",
    "        temp: temperature parameter\n",
    "    \"\"\"\n",
    "    # preds/targetsã¯2æ¬¡å…ƒ (batch, emb_dim) ã‚’æƒ³å®š\n",
    "    # ã‚‚ã—3æ¬¡å…ƒãªã‚‰æœ€å¾Œã®æ¬¡å…ƒä»¥å¤–ã‚’flatten\n",
    "    if preds.dim() == 3:\n",
    "        preds = preds.reshape(preds.shape[0], -1)\n",
    "    if targets.dim() == 3:\n",
    "        targets = targets.reshape(targets.shape[0], -1)\n",
    "    \n",
    "    # æ¬¡å…ƒãŒä¸€è‡´ã—ãªã„å ´åˆã¯predsã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆç·Šæ€¥å¯¾å¿œï¼‰\n",
    "    if preds.shape[1] != targets.shape[1]:\n",
    "        print(f\"Warning: dimension mismatch - preds:{preds.shape[1]}, targets:{targets.shape[1]}\")\n",
    "        # CLIPã®æ¬¡å…ƒã«åˆã‚ã›ã¦ç·šå½¢å¤‰æ›ï¼ˆå­¦ç¿’å¯èƒ½ã«ã™ã‚‹å ´åˆã¯è¦æ¤œè¨ï¼‰\n",
    "        if not hasattr(soft_clip_loss, 'dim_adapter'):\n",
    "            soft_clip_loss.dim_adapter = nn.Linear(preds.shape[1], targets.shape[1]).to(preds.device)\n",
    "        preds = soft_clip_loss.dim_adapter(preds)\n",
    "    \n",
    "    # å…ƒã®utils.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯\n",
    "    clip_clip = (targets @ targets.T) / temp\n",
    "    brain_clip = (preds @ targets.T) / temp\n",
    "    \n",
    "    loss1 = -(brain_clip.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "    loss2 = -(brain_clip.T.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "    \n",
    "    return (loss1 + loss2) / 2\n",
    "\n",
    "def train_step(batch):\n",
    "    \"\"\"1ãƒãƒƒãƒã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "    fmri = batch['fmri'].to(DEVICE)\n",
    "    images = batch['image'].to(DEVICE)\n",
    "\n",
    "    # CLIPç‰¹å¾´ã‚’å–å¾—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰\n",
    "    with torch.no_grad():\n",
    "        clip_target = get_clip_features(images)  # (batch, emb_dim)\n",
    "\n",
    "    # Forward\n",
    "    backbone, clip_voxels, blurry = model(fmri)\n",
    "\n",
    "    # å½¢çŠ¶ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®ãƒãƒƒãƒã®ã¿ï¼‰\n",
    "    if not hasattr(train_step, 'debug_printed'):\n",
    "        print(f\"Debug shapes:\")\n",
    "        print(f\"  fmri: {fmri.shape}\")\n",
    "        print(f\"  backbone: {backbone.shape}\")\n",
    "        print(f\"  clip_voxels: {clip_voxels.shape}\")\n",
    "        print(f\"  clip_target: {clip_target.shape}\")\n",
    "        train_step.debug_printed = True\n",
    "\n",
    "    # clip_voxelsã‚’flattenã—ã¦ä½¿ã†\n",
    "    # clip_voxels: (batch, seq_len, clip_emb_dim) -> (batch, seq_len * clip_emb_dim)\n",
    "    clip_voxels_flat = clip_voxels.reshape(clip_voxels.shape[0], -1)\n",
    "    clip_target_flat = clip_target.reshape(clip_target.shape[0], -1)\n",
    "\n",
    "    # Lossè¨ˆç®—\n",
    "    loss = soft_clip_loss(clip_voxels_flat, clip_target_flat)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d188f2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d188f2f",
    "outputId": "52fa9357-6fd9-4259-da49-5b7ccb10e196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 1.06 GB allocated\n",
      "GPU Memory: 1.10 GB reserved\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¡ãƒ¢ãƒªç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51e0367f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "d0dfe203d23041b9aeede9f0e54bd3e7",
      "69e934c6520a4fd0a724de7df1039aa6",
      "eb163e2004d8414c93af1e6961ce037a",
      "cd7340b752ea407cbc71ff3c8c05c0f2",
      "394d268177c7432db1d1f2c98b2990d8",
      "285ba4c598834b5b8b35babe0cfa09ce",
      "18fca8fde273464bbebfa68a15c12147",
      "42814c33b28d4295a0658c9985f5eb88",
      "9dea039f804742099391c34f85938ec6",
      "ebb8a67534a84a0cb2a244dde0e787fb",
      "e12e2de99cc44400a2728ef893ddef6d"
     ]
    },
    "id": "51e0367f",
    "outputId": "57cb5dbf-3154-4867-992a-9f0acb0f7d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting training: 1 epochs, 4920 batches/epoch\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dfe203d23041b9aeede9f0e54bd3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/4920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x425984 and 512x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2470480545.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{loss:.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-639488239.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mclip_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_clip_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_voxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-639488239.py\u001b[0m in \u001b[0;36msoft_clip_loss\u001b[0;34m(preds, targets, temp)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x425984 and 512x2)"
     ]
    }
   ],
   "source": [
    "# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_loader)} batches/epoch\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        loss = train_step(batch)\n",
    "        epoch_losses.append(loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ãƒ¡ãƒ¢ãƒªç¢ºèª\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf52af",
   "metadata": {
    "id": "f1cf52af"
   },
   "outputs": [],
   "source": [
    "# æå¤±ã®å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e4d2f",
   "metadata": {
    "id": "f70e4d2f"
   },
   "source": [
    "## 7. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b93a67",
   "metadata": {
    "id": "39b93a67"
   },
   "outputs": [],
   "source": [
    "from transfer_utils import save_checkpoint\n",
    "\n",
    "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "save_dir = os.path.join(CHECKPOINT_DIR, f\"algonauts_{SUBJECT}_{TRAIN_MODE}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=NUM_EPOCHS,\n",
    "    save_path=os.path.join(save_dir, \"last.pth\"),\n",
    "    extra_info={\n",
    "        \"train_mode\": TRAIN_MODE,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"final_loss\": losses[-1] if losses else None,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"\\nCheckpoint saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c46259",
   "metadata": {
    "id": "d2c46259"
   },
   "source": [
    "## 8. ç°¡æ˜“æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dee4e2",
   "metadata": {
    "id": "38dee4e2"
   },
   "outputs": [],
   "source": [
    "# æ¨è«–ãƒ†ã‚¹ãƒˆ\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_fmri = test_batch['fmri'].to(DEVICE)\n",
    "    test_images = test_batch['image'].to(DEVICE)\n",
    "\n",
    "    # fMRI â†’ CLIP tokens\n",
    "    backbone, clip_voxels, blurry = model(test_fmri)\n",
    "\n",
    "    # å®Ÿéš›ã®CLIPç‰¹å¾´\n",
    "    clip_target = get_clip_features(test_images)\n",
    "    if clip_target.dim() == 2:\n",
    "        clip_target = clip_target.unsqueeze(1)\n",
    "\n",
    "    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "    pred_flat = F.normalize(clip_voxels.view(clip_voxels.shape[0], -1), dim=-1)\n",
    "    target_flat = F.normalize(clip_target.view(clip_target.shape[0], -1), dim=-1)\n",
    "\n",
    "    similarity = (pred_flat * target_flat).sum(dim=-1).mean()\n",
    "\n",
    "    print(f\"\\nInference test:\")\n",
    "    print(f\"  Input fMRI shape: {test_fmri.shape}\")\n",
    "    print(f\"  Output CLIP shape: {clip_voxels.shape}\")\n",
    "    print(f\"  Average cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962d6be",
   "metadata": {
    "id": "d962d6be"
   },
   "outputs": [],
   "source": [
    "# å…¥åŠ›ç”»åƒã®å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"ImageNetæ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\n",
    "sample_images = denormalize(test_images[:4])\n",
    "grid = make_grid(sample_images, nrow=4).cpu().permute(1, 2, 0).numpy()\n",
    "grid = np.clip(grid, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(grid)\n",
    "plt.title(\"Sample Training Images\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5bbd3",
   "metadata": {
    "id": "f9d5bbd3"
   },
   "source": [
    "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "1. **ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œç¢ºèª** â†’ ã‚¨ãƒ©ãƒ¼ãªãå®Œäº†ã™ã‚Œã°OK\n",
    "2. **è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆlightï¼‰ã§å®Ÿå­¦ç¿’** â†’ T4ã§æ•°æ™‚é–“\n",
    "3. **æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼ˆstandardï¼‰ã§æœ¬æ ¼å­¦ç¿’** â†’ Pro or ç ”ç©¶å®¤PC\n",
    "4. **æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯** â†’ `mindeye_inference_colab.ipynb` ã§ç”»åƒå†æ§‹æˆ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1369be7e088f48c38298e34d674dd052": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18fca8fde273464bbebfa68a15c12147": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2067b4ce007648cd93ebfa662e873155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24507560358f462cb8021f8f4ecb42ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285ba4c598834b5b8b35babe0cfa09ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d3f8231f66a4d83b53e90ff2c6a9363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37947142af5443ef9a8c764cb956e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "394d268177c7432db1d1f2c98b2990d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42814c33b28d4295a0658c9985f5eb88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56d8754cfee447a8a4bbe1c95a6e4ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24507560358f462cb8021f8f4ecb42ba",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ca7a99b3f5fa4c17b1edadd0f0321314",
      "value": "open_clip_model.safetensors:â€‡100%"
     }
    },
    "650bdbc5a5d744039bf6f7c9aedf59c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69e934c6520a4fd0a724de7df1039aa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_285ba4c598834b5b8b35babe0cfa09ce",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_18fca8fde273464bbebfa68a15c12147",
      "value": "Epochâ€‡1/1:â€‡â€‡â€‡0%"
     }
    },
    "6e5ec983c10741ee87e5fefd371cc748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1369be7e088f48c38298e34d674dd052",
      "max": 605143316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d3f8231f66a4d83b53e90ff2c6a9363",
      "value": 605143316
     }
    },
    "7996b0610b624a289add344d00a72d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2067b4ce007648cd93ebfa662e873155",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_37947142af5443ef9a8c764cb956e90c",
      "value": "â€‡605M/605Mâ€‡[00:06&lt;00:00,â€‡178MB/s]"
     }
    },
    "9dea039f804742099391c34f85938ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bc9b5a8cbd3a4119941e9e7b93455500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56d8754cfee447a8a4bbe1c95a6e4ee1",
       "IPY_MODEL_6e5ec983c10741ee87e5fefd371cc748",
       "IPY_MODEL_7996b0610b624a289add344d00a72d9f"
      ],
      "layout": "IPY_MODEL_650bdbc5a5d744039bf6f7c9aedf59c6"
     }
    },
    "ca7a99b3f5fa4c17b1edadd0f0321314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd7340b752ea407cbc71ff3c8c05c0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb8a67534a84a0cb2a244dde0e787fb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e12e2de99cc44400a2728ef893ddef6d",
      "value": "â€‡0/4920â€‡[00:01&lt;?,â€‡?it/s]"
     }
    },
    "d0dfe203d23041b9aeede9f0e54bd3e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69e934c6520a4fd0a724de7df1039aa6",
       "IPY_MODEL_eb163e2004d8414c93af1e6961ce037a",
       "IPY_MODEL_cd7340b752ea407cbc71ff3c8c05c0f2"
      ],
      "layout": "IPY_MODEL_394d268177c7432db1d1f2c98b2990d8"
     }
    },
    "e12e2de99cc44400a2728ef893ddef6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb163e2004d8414c93af1e6961ce037a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42814c33b28d4295a0658c9985f5eb88",
      "max": 4920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9dea039f804742099391c34f85938ec6",
      "value": 0
     }
    },
    "ebb8a67534a84a0cb2a244dde0e787fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

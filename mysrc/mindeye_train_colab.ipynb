{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e04e0d",
   "metadata": {
    "id": "30e04e0d"
   },
   "source": [
    "## 1. Ë®≠ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d469dc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dalle2-pytorch\n",
      "Version: 1.15.6\n",
      "Summary: DALL-E 2\n",
      "Home-page: https://github.com/lucidrains/dalle2-pytorch\n",
      "Author: Phil Wang\n",
      "Author-email: lucidrains@gmail.com\n",
      "License: MIT\n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: accelerate, click, clip-anytorch, coca-pytorch, einops, ema-pytorch, embedding-reader, fsspec, kornia, numpy, open-clip-torch, packaging, pillow, pydantic, pytorch-warmup, resize-right, rotary-embedding-torch, torch, torchmetrics, torchvision, tqdm, vector-quantize-pytorch, webdataset, x-clip\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show dalle2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a038c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: clip\n",
      "Version: 1.0\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: OpenAI\n",
      "Author-email: \n",
      "License: \n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: ftfy, packaging, regex, torch, torchvision, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e763902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1d8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"lab\"#\"colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98da37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 1. src„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí„Éë„Çπ„Å´ËøΩÂä† (ImportError: Could not import BrainNetwork ÂØæÁ≠ñ)\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(current_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# 2. torch.xpu „Ç®„É©„ÉºÂØæÁ≠ñ\n",
    "if not hasattr(torch, \"xpu\"):\n",
    "    class MockXPU:\n",
    "        def empty_cache(self): pass\n",
    "        def device_count(self): return 0\n",
    "        def manual_seed(self, seed): pass\n",
    "    torch.xpu = MockXPU()\n",
    "\n",
    "# „Åì„ÅÆÂæå„Å´ import „ÇíË°å„ÅÜ\n",
    "from models_algonauts import AlgonautsMindEye, create_algonauts_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f18df81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f18df81",
    "outputId": "d9856e11-1c3f-46ac-ff52-1a1f88a6c309"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Google Drive„Çí„Éû„Ç¶„É≥„ÉàÔºàÊó¢„Å´„Éû„Ç¶„É≥„ÉàÊ∏à„Åø„ÅÆÂ†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÔºâ\n",
    "\n",
    "# „Éû„Ç¶„É≥„Éà\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# zip„Éï„Ç°„Ç§„É´„ÅÆ„Éë„ÇπÔºà‰æãÔºöGoogle Drive‰∏ä„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂÖàÔºâ\n",
    "zip_path = \"/content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip\"  # Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "\n",
    "# Â±ïÈñãÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™\n",
    "extract_path = \"/content/data/algonauts_2023_challenge_data/train_data/\"  # Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "\n",
    "# zip„Éï„Ç°„Ç§„É´„ÅÆÁ¢∫Ë™ç„Å®Â±ïÈñã\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"Extracting {zip_path} to {extract_path}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(f\"Zip file not found at: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de1933e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1de1933e",
    "outputId": "f376733f-5873-4edb-8e4d-0188e90ce00a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: dummy\n",
      "Hidden Dim: 256\n",
      "Batch Size: 2\n",
      "Epochs: 1\n",
      "Dummy Mode: True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Â≠¶ÁøíË®≠ÂÆö - ÂøÖË¶Å„Å´Âøú„Åò„Å¶Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "# =============================================================================\n",
    "\n",
    "# Â≠¶Áøí„É¢„Éº„Éâ: \"dummy\" | \"light\" | \"standard\"\n",
    "TRAIN_MODE = \"dummy\"\n",
    "\n",
    "# Ë¢´È®ìËÄÖID\n",
    "SUBJECT = \"subj01\"\n",
    "\n",
    "# „Éá„Éº„Çø„Éë„ÇπÔºàGoogle Drive‰∏äÔºâ\n",
    "DATA_ROOT = \"/mount/nfs6/visitor/nsd/algonauts_2023_challenge_data/train_data\"#\"/content/data/algonauts_2023_challenge_data/train_data/\"\n",
    "\n",
    "# „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠òÂÖà\n",
    "CHECKPOINT_DIR = \"/mount/nfs6/visitor/nsd/folder/mindeye_checkpoints\"#\"/content/data/mindeye_checkpoints\"\n",
    "\n",
    "# Êó¢Â≠ò„ÅÆÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´ÔºàËª¢ÁßªÂ≠¶ÁøíÁî®„ÄÅNone„ÅÆÂ†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÔºâ\n",
    "PRETRAINED_CKPT = None  # ‰æã: \"/content/drive/MyDrive/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
    "\n",
    "# =============================================================================\n",
    "# „É¢„Éº„ÉâÂà•Ë®≠ÂÆöÔºàËá™ÂãïË®≠ÂÆöÔºâ\n",
    "# =============================================================================\n",
    "if TRAIN_MODE == \"dummy\":\n",
    "    HIDDEN_DIM = 256\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 1\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = True\n",
    "elif TRAIN_MODE == \"light\":\n",
    "    HIDDEN_DIM = 512\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    USE_PRIOR = False\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "else:  # standard\n",
    "    HIDDEN_DIM = 1024\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    USE_PRIOR = True\n",
    "    BLURRY_RECON = False\n",
    "    DUMMY_MODE = False\n",
    "\n",
    "print(f\"Mode: {TRAIN_MODE}\")\n",
    "print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Dummy Mode: {DUMMY_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892613",
   "metadata": {
    "id": "9f892613"
   },
   "source": [
    "## 2. Áí∞Â¢ÉÊßãÁØâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fef90d8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fef90d8a",
    "outputId": "07539b64-de84-4e32-e606-fd942ca07579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 13 19:23:47 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             40W /  300W |      17MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             44W /  300W |      17MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2164      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A            2164      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU„ÅÆÁ¢∫Ë™ç\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a979c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a979c8",
    "outputId": "df42c61f-2e73-4f93-e43f-d79380da228f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úì DUMMY_MODE: Skipping heavy libraries (dalle2-pytorch, kornia, pytorch_lightning)\n"
     ]
    }
   ],
   "source": [
    "# Âü∫Êú¨„Éë„ÉÉ„Ç±„Éº„Ç∏ÔºàÂ∏∏„Å´ÂøÖË¶ÅÔºâ\n",
    "!pip install -q torch torchvision tqdm einops h5py pillow matplotlib requests numpy\n",
    "!pip install -q omegaconf accelerate\n",
    "\n",
    "# OpenAI CLIPÔºàsrc/models.py„ÅåÂøÖË¶ÅÔºâ\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# webdatasetÔºàsrc/utils.py„ÅåÂøÖË¶ÅÔºâ\n",
    "!pip install -q webdataset\n",
    "\n",
    "# OpenCLIP\n",
    "!pip install -q open_clip_torch\n",
    "\n",
    "# ‚ö†Ô∏è DUMMY_MODE=True„ÅÆÂ†¥Âêà„ÅØÈáç„ÅÑ„É©„Ç§„Éñ„É©„É™„Çí„Çπ„Ç≠„ÉÉ„Éó\n",
    "if DUMMY_MODE:\n",
    "    print(\"‚úì DUMMY_MODE: Skipping heavy libraries (dalle2-pytorch, kornia, pytorch_lightning)\")\n",
    "else:\n",
    "    # Êú¨Áï™„É¢„Éº„ÉâÁî®„ÅÆÈáç„ÅÑ„Éë„ÉÉ„Ç±„Éº„Ç∏\n",
    "    !pip install -q pytorch_lightning\n",
    "    !pip install -q kornia\n",
    "    !pip install -q dalle2-pytorch\n",
    "    !pip install -q diffusers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1062b8c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1062b8c4",
    "outputId": "6e739e11-1a4d-4120-e468-5d884de0c63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: could not create leading directories of '/content/MindEyeV2': Ë®±ÂèØ„Åå„ÅÇ„Çä„Åæ„Åõ„Çì\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/MindEyeV2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/content/MindEyeV2/mysrc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/content/MindEyeV2/src\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/content/MindEyeV2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorking directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/MindEyeV2'"
     ]
    }
   ],
   "source": [
    "# „É™„Éù„Ç∏„Éà„É™„ÅÆ„ÇØ„É≠„Éº„É≥ÔºàÊó¢Â≠ò„ÅÆÂ†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÔºâ\n",
    "import os\n",
    "if not os.path.exists(\"/content/MindEyeV2\"):\n",
    "    # TODO: Ëá™ÂàÜ„ÅÆ„Éï„Ç©„Éº„ÇØ„Åó„Åü„É™„Éù„Ç∏„Éà„É™URL„Å´Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "    !git clone https://github.com/boxed-mikann/MindEyeV2.git /content/MindEyeV2\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "# „Éë„Çπ„ÇíËøΩÂä†\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
    "\n",
    "os.chdir(\"/content/MindEyeV2\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d79e285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/pcnl_guest3/workspace/MindEyeV2\n"
     ]
    }
   ],
   "source": [
    "# „Éë„Çπ„ÇíËøΩÂä†\n",
    "import sys ,os\n",
    "\n",
    "sys.path.insert(0, \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc\")\n",
    "sys.path.insert(0, \"/home/pcnl_guest3/workspace/MindEyeV2/src\")\n",
    "\n",
    "os.chdir(\"//home/pcnl_guest3/workspace/MindEyeV2\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "StGS0JkILkyW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StGS0JkILkyW",
    "outputId": "a707a38a-26fe-4719-b67a-c71ea12fc9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling latest changes from MindEyeV2 repository...\n",
      "Already up to date.\n",
      "Repository update complete.\n"
     ]
    }
   ],
   "source": [
    "print('Pulling latest changes from MindEyeV2 repository...')\n",
    "!git pull\n",
    "print('Repository update complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6985daca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6985daca",
    "outputId": "8d4288ca-a5ac-42b7-cdd0-173671013d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data found: /mount/nfs6/visitor/nsd/algonauts_2023_challenge_data/train_data/subj01\n",
      "ÂêàË®à 20\n",
      "drwxrwxr-x  5 pcnl_guest3 visitor 4096  1Êúà 11  2023 .\n",
      "drwxrwxr-x 10 shinji      visitor 4096 12Êúà 16 17:31 ..\n",
      "drwxr-x---  2 pcnl_guest3 visitor 4096 12Êúà 10  2022 roi_masks\n",
      "drwxr-x---  3 pcnl_guest3 visitor 4096 12Êúà 10  2022 test_split\n",
      "drwxr-x---  4 pcnl_guest3 visitor 4096 10Êúà 31  2022 training_split\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# „Éá„Éº„Çø„ÅÆÂ≠òÂú®Á¢∫Ë™ç\n",
    "import os\n",
    "subj_dir = os.path.join(DATA_ROOT, SUBJECT)\n",
    "if os.path.exists(subj_dir):\n",
    "    print(f\"‚úì Data found: {subj_dir}\")\n",
    "    !ls -la {subj_dir}\n",
    "else:\n",
    "    print(f\"‚úó Data NOT found: {subj_dir}\")\n",
    "    print(\"Please upload Algonauts2023 data to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf51bdb",
   "metadata": {
    "id": "2bf51bdb"
   },
   "source": [
    "## 3. „Éá„Éº„ÇøË™≠„ÅøËæº„Åø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f3bafa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f3bafa2",
    "outputId": "026509ff-274c-4047-9800-9e7ead94ffe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MindEyeV2 Algonauts Configuration\n",
      "============================================================\n",
      "Environment:     Local\n",
      "Device:          cuda\n",
      "Dummy Mode:      True\n",
      "Data Root:       D:/data/algonauts_2023_challenge_data\n",
      "Checkpoint Dir:  /home/pcnl_guest3/workspace/MindEyeV2/train_logs\n",
      "Output Dir:      /home/pcnl_guest3/workspace/MindEyeV2/outputs\n",
      "SRC Dir:         /home/pcnl_guest3/workspace/MindEyeV2/src\n",
      "============================================================\n",
      "\n",
      "Device: cuda\n",
      "Total vertices for subj01: 39548\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Áí∞Â¢ÉÂ§âÊï∞„Åß„ÉÄ„Éü„Éº„É¢„Éº„Éâ„ÇíË®≠ÂÆö\n",
    "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
    "\n",
    "# mysrc„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà\n",
    "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
    "from config import print_config, DEVICE\n",
    "\n",
    "print_config()\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Total vertices for {SUBJECT}: {get_total_vertices(SUBJECT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0pp7KTZjWpDh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pp7KTZjWpDh",
    "outputId": "126e792e-9741-4044-8817-e492bcd8b470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä Memory Usage BEFORE Model Creation\n",
      "============================================================\n",
      "  cpu_mb: 3517.15 MB\n",
      "  gpu_allocated_mb: 0.00 MB\n",
      "  gpu_reserved_mb: 0.00 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üîç „É°„É¢„É™„Éá„Éê„ÉÉ„Ç∞ - „É¢„Éá„É´‰ΩúÊàêÂâç\n",
    "# =============================================================================\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_info():\n",
    "    \"\"\"CPU/GPU„É°„É¢„É™ÊÉÖÂ†±„ÇíË°®Á§∫\"\"\"\n",
    "    # CPU„É°„É¢„É™\n",
    "    process = psutil.Process()\n",
    "    cpu_mem_mb = process.memory_info().rss / 1024**2\n",
    "\n",
    "    # GPU„É°„É¢„É™\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_mb = torch.cuda.memory_allocated() / 1024**2\n",
    "        gpu_reserved_mb = torch.cuda.memory_reserved() / 1024**2\n",
    "        return {\n",
    "            \"cpu_mb\": cpu_mem_mb,\n",
    "            \"gpu_allocated_mb\": gpu_mem_mb,\n",
    "            \"gpu_reserved_mb\": gpu_reserved_mb,\n",
    "        }\n",
    "    return {\"cpu_mb\": cpu_mem_mb}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä Memory Usage BEFORE Model Creation\")\n",
    "print(\"=\"*60)\n",
    "mem_before = get_memory_info()\n",
    "for k, v in mem_before.items():\n",
    "    print(f\"  {k}: {v:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83223b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.9.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, clip, clip-anytorch, CoCa-pytorch, dalle2-pytorch, deepspeed, ema-pytorch, kornia, open-clip-torch, pytorch-lightning, pytorch-warmup, rotary-embedding-torch, sentence-transformers, timm, torch-fidelity, torchmetrics, torchvision, vector-quantize-pytorch, x-clip, xformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "936f9ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "936f9ce3",
    "outputId": "3e768d6e-66c0-4469-c7a5-b741162c2176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fMRI data for subj01...\n",
      "  LH shape: (9841, 19004), RH shape: (9841, 20544)\n",
      "  Combined shape: (9841, 39548)\n",
      "Found 9841 train images\n",
      "\n",
      "Dataset size: 9841\n",
      "Number of batches: 4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x145efc4de5c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._rcvd_idx += 1\n",
      "Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x145efc4de5c0>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "^^    ^^self._rcvd_idx += 1^\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "^^    if (^\n",
      "^  ^ ^  ^ ^ ^^^^\n",
      "^  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "       if (  \n",
      "          ^^ ^ ^^ ^^^^^^\n",
      "^  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "^ ^^ ^ ^ ^^ ^ ^ ^ ^^ ^ ^ ^^^\n",
      "^AssertionError: ^can only test a child process^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    # `None`.\n              \n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    else:\n          \n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    else:\n          \n  File \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc/algonauts_dataset.py\", line 241, in __getitem__\n    result[\"image\"] = self.transform(image)\n                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc/algonauts_dataset.py\", line 90, in default_image_transform\n    from torchvision import transforms\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/__init__.py\", line 10, in <module>\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 2, in <module>\n    from .convnext import *\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/models/convnext.py\", line 9, in <module>\n    from ..ops.misc import Conv2dNormActivation, Permute\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/ops/__init__.py\", line 1, in <module>\n    from ._register_onnx_ops import _register_custom_op\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/ops/_register_onnx_ops.py\", line 5, in <module>\n    from torch.onnx import symbolic_opset11 as opset11\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/__init__.py\", line 27, in <module>\n    from . import errors, ops\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/__init__.py\", line 23, in <module>\n    from torch.onnx.ops import _impl, _symbolic_impl\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/_impl.py\", line 7, in <module>\n    from torch.onnx.ops import _dtype_mappings\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/_dtype_mappings.py\", line 8, in <module>\n    4: torch.uint16,  # UINT16\n       ^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/__init__.py\", line 1833, in __getattr__\n    ################################################################################\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: module 'torch' has no attribute 'uint16'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# „Çµ„É≥„Éó„É´„Éê„ÉÉ„ÉÅ„ÇíÂèñÂæó\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m sample_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSample batch:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  fMRI shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_batch[\u001b[33m'\u001b[39m\u001b[33mfmri\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1344\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1369\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/_utils.py:694\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    691\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mAttributeError\u001b[39m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    # `None`.\n              \n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    else:\n          \n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    else:\n          \n  File \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc/algonauts_dataset.py\", line 241, in __getitem__\n    result[\"image\"] = self.transform(image)\n                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/workspace/MindEyeV2/mysrc/algonauts_dataset.py\", line 90, in default_image_transform\n    from torchvision import transforms\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/__init__.py\", line 10, in <module>\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 2, in <module>\n    from .convnext import *\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/models/convnext.py\", line 9, in <module>\n    from ..ops.misc import Conv2dNormActivation, Permute\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/ops/__init__.py\", line 1, in <module>\n    from ._register_onnx_ops import _register_custom_op\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torchvision/ops/_register_onnx_ops.py\", line 5, in <module>\n    from torch.onnx import symbolic_opset11 as opset11\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/__init__.py\", line 27, in <module>\n    from . import errors, ops\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/__init__.py\", line 23, in <module>\n    from torch.onnx.ops import _impl, _symbolic_impl\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/_impl.py\", line 7, in <module>\n    from torch.onnx.ops import _dtype_mappings\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/onnx/ops/_dtype_mappings.py\", line 8, in <module>\n    4: torch.uint16,  # UINT16\n       ^^^^^^^^^^^^\n  File \"/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/torch/__init__.py\", line 1833, in __getattr__\n    ################################################################################\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: module 'torch' has no attribute 'uint16'\n"
     ]
    }
   ],
   "source": [
    "# „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº‰ΩúÊàê\n",
    "import numpy\n",
    "train_loader = get_dataloader(\n",
    "    data_root=DATA_ROOT,\n",
    "    subject=SUBJECT,\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# „Çµ„É≥„Éó„É´„Éê„ÉÉ„ÉÅ„ÇíÂèñÂæó\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  fMRI shape: {sample_batch['fmri'].shape}\")\n",
    "print(f\"  Image shape: {sample_batch['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3882cc",
   "metadata": {
    "id": "6e3882cc"
   },
   "source": [
    "## 4. „É¢„Éá„É´‰ΩúÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ddf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: huggingface-hub\n",
      "Version: 0.36.0\n",
      "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
      "Home-page: https://github.com/huggingface/huggingface_hub\n",
      "Author: Hugging Face, Inc.\n",
      "Author-email: julien@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, hf-xet, packaging, pyyaml, requests, tqdm, typing-extensions\n",
      "Required-by: accelerate, datasets, diffusers, evaluate, open-clip-torch, sentence-transformers, timm, tokenizers, transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.57.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b736f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: diffusers\n",
      "Version: 0.36.0\n",
      "Summary: State-of-the-art diffusion in PyTorch and JAX.\n",
      "Home-page: https://github.com/huggingface/diffusers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/diffusers/graphs/contributors)\n",
      "Author-email: diffusers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages\n",
      "Requires: filelock, httpx, huggingface-hub, importlib_metadata, numpy, Pillow, regex, requests, safetensors\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d2104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f9d2104",
    "outputId": "4d608ab8-fdbf-4194-bc5f-f5a267537fde"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: cannot import name 'DEFAULT_HF_PARALLEL_LOADING_WORKERS' from 'diffusers.utils.constants' (/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/constants.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/models.py:15\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decoder\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     DIFFUSERS_SLOW_IMPORT,\n\u001b[32m      7\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m      8\u001b[39m     _LazyModule,\n\u001b[32m      9\u001b[39m     is_accelerate_available,\n\u001b[32m     10\u001b[39m     is_bitsandbytes_available,\n\u001b[32m     11\u001b[39m     is_flax_available,\n\u001b[32m     12\u001b[39m     is_gguf_available,\n\u001b[32m     13\u001b[39m     is_k_diffusion_available,\n\u001b[32m     14\u001b[39m     is_librosa_available,\n\u001b[32m     15\u001b[39m     is_note_seq_available,\n\u001b[32m     16\u001b[39m     is_nvidia_modelopt_available,\n\u001b[32m     17\u001b[39m     is_onnx_available,\n\u001b[32m     18\u001b[39m     is_opencv_available,\n\u001b[32m     19\u001b[39m     is_optimum_quanto_available,\n\u001b[32m     20\u001b[39m     is_scipy_available,\n\u001b[32m     21\u001b[39m     is_sentencepiece_available,\n\u001b[32m     22\u001b[39m     is_torch_available,\n\u001b[32m     23\u001b[39m     is_torchao_available,\n\u001b[32m     24\u001b[39m     is_torchsde_available,\n\u001b[32m     25\u001b[39m     is_transformers_available,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/__init__.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     CONFIG_NAME,\n\u001b[32m     23\u001b[39m     DEFAULT_HF_PARALLEL_LOADING_WORKERS,\n\u001b[32m     24\u001b[39m     DEPRECATED_REVISION_ARGS,\n\u001b[32m     25\u001b[39m     DIFFUSERS_DYNAMIC_MODULE_NAME,\n\u001b[32m     26\u001b[39m     FLAX_WEIGHTS_NAME,\n\u001b[32m     27\u001b[39m     GGUF_FILE_EXTENSION,\n\u001b[32m     28\u001b[39m     HF_ENABLE_PARALLEL_LOADING,\n\u001b[32m     29\u001b[39m     HF_MODULES_CACHE,\n\u001b[32m     30\u001b[39m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[32m     31\u001b[39m     MIN_PEFT_VERSION,\n\u001b[32m     32\u001b[39m     ONNX_EXTERNAL_WEIGHTS_NAME,\n\u001b[32m     33\u001b[39m     ONNX_WEIGHTS_NAME,\n\u001b[32m     34\u001b[39m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[32m     35\u001b[39m     SAFETENSORS_FILE_EXTENSION,\n\u001b[32m     36\u001b[39m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[32m     37\u001b[39m     USE_PEFT_BACKEND,\n\u001b[32m     38\u001b[39m     WEIGHTS_INDEX_NAME,\n\u001b[32m     39\u001b[39m     WEIGHTS_NAME,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _maybe_remap_transformers_class, deprecate\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DEFAULT_HF_PARALLEL_LOADING_WORKERS' from 'diffusers.utils.constants' (/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/constants.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:168\u001b[39m, in \u001b[36mAlgonautsMindEye.__init__\u001b[39m\u001b[34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrainNetwork\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = BrainNetwork(\n\u001b[32m    170\u001b[39m         h=hidden_dim,\n\u001b[32m    171\u001b[39m         in_dim=hidden_dim,  \u001b[38;5;66;03m# RidgeÂá∫Âäõ„Å®Âêå„Åò\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m         clip_scale=clip_scale,\n\u001b[32m    179\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/src/models.py:17\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoencoders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decoder\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBrainNetwork\u001b[39;00m(nn.Module):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     DIFFUSERS_SLOW_IMPORT,\n\u001b[32m      7\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m      8\u001b[39m     _LazyModule,\n\u001b[32m      9\u001b[39m     is_accelerate_available,\n\u001b[32m     10\u001b[39m     is_bitsandbytes_available,\n\u001b[32m     11\u001b[39m     is_flax_available,\n\u001b[32m     12\u001b[39m     is_gguf_available,\n\u001b[32m     13\u001b[39m     is_k_diffusion_available,\n\u001b[32m     14\u001b[39m     is_librosa_available,\n\u001b[32m     15\u001b[39m     is_note_seq_available,\n\u001b[32m     16\u001b[39m     is_nvidia_modelopt_available,\n\u001b[32m     17\u001b[39m     is_onnx_available,\n\u001b[32m     18\u001b[39m     is_opencv_available,\n\u001b[32m     19\u001b[39m     is_optimum_quanto_available,\n\u001b[32m     20\u001b[39m     is_scipy_available,\n\u001b[32m     21\u001b[39m     is_sentencepiece_available,\n\u001b[32m     22\u001b[39m     is_torch_available,\n\u001b[32m     23\u001b[39m     is_torchao_available,\n\u001b[32m     24\u001b[39m     is_torchsde_available,\n\u001b[32m     25\u001b[39m     is_transformers_available,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/__init__.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     CONFIG_NAME,\n\u001b[32m     23\u001b[39m     DEFAULT_HF_PARALLEL_LOADING_WORKERS,\n\u001b[32m     24\u001b[39m     DEPRECATED_REVISION_ARGS,\n\u001b[32m     25\u001b[39m     DIFFUSERS_DYNAMIC_MODULE_NAME,\n\u001b[32m     26\u001b[39m     FLAX_WEIGHTS_NAME,\n\u001b[32m     27\u001b[39m     GGUF_FILE_EXTENSION,\n\u001b[32m     28\u001b[39m     HF_ENABLE_PARALLEL_LOADING,\n\u001b[32m     29\u001b[39m     HF_MODULES_CACHE,\n\u001b[32m     30\u001b[39m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[32m     31\u001b[39m     MIN_PEFT_VERSION,\n\u001b[32m     32\u001b[39m     ONNX_EXTERNAL_WEIGHTS_NAME,\n\u001b[32m     33\u001b[39m     ONNX_WEIGHTS_NAME,\n\u001b[32m     34\u001b[39m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[32m     35\u001b[39m     SAFETENSORS_FILE_EXTENSION,\n\u001b[32m     36\u001b[39m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[32m     37\u001b[39m     USE_PEFT_BACKEND,\n\u001b[32m     38\u001b[39m     WEIGHTS_INDEX_NAME,\n\u001b[32m     39\u001b[39m     WEIGHTS_NAME,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _maybe_remap_transformers_class, deprecate\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'DEFAULT_HF_PARALLEL_LOADING_WORKERS' from 'diffusers.utils.constants' (/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/constants.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransfer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     load_pretrained_without_ridge,\n\u001b[32m      4\u001b[39m     freeze_layers,\n\u001b[32m      5\u001b[39m     get_trainable_params,\n\u001b[32m      6\u001b[39m     print_parameter_summary,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# „É¢„Éá„É´‰ΩúÊàê\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mcreate_algonauts_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSUBJECT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_PRIOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLURRY_RECON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel created!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m print_parameter_summary(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:304\u001b[39m, in \u001b[36mcreate_algonauts_model\u001b[39m\u001b[34m(subjects, hidden_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, device)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_algonauts_model\u001b[39m(\n\u001b[32m    277\u001b[39m     subjects: List[\u001b[38;5;28mstr\u001b[39m] = [\u001b[33m\"\u001b[39m\u001b[33msubj01\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    278\u001b[39m     hidden_dim: \u001b[38;5;28mint\u001b[39m = \u001b[32m1024\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m     device: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    286\u001b[39m ) -> AlgonautsMindEye:\n\u001b[32m    287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    AlgonautsÂØæÂøú„É¢„Éá„É´„Çí‰ΩúÊàê\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m \u001b[33;03m        AlgonautsMindEye „Ç§„É≥„Çπ„Çø„É≥„Çπ\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     model = \u001b[43mAlgonautsMindEye\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_emb_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_emb_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_seq_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_seq_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_prior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblurry_recon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/MindEyeV2/mysrc/models_algonauts.py:181\u001b[39m, in \u001b[36mAlgonautsMindEye.__init__\u001b[39m\u001b[34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[39m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = BrainNetwork(\n\u001b[32m    170\u001b[39m         h=hidden_dim,\n\u001b[32m    171\u001b[39m         in_dim=hidden_dim,  \u001b[38;5;66;03m# RidgeÂá∫Âäõ„Å®Âêå„Åò\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m         clip_scale=clip_scale,\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import BrainNetwork from src/models.py. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMake sure src/ is in your Python path. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Diffusion PriorÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m.diffusion_prior = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: cannot import name 'DEFAULT_HF_PARALLEL_LOADING_WORKERS' from 'diffusers.utils.constants' (/home/pcnl_guest3/miniforge3/envs/fmri3/lib/python3.11/site-packages/diffusers/utils/constants.py)"
     ]
    }
   ],
   "source": [
    "from models_algonauts import AlgonautsMindEye, create_algonauts_model\n",
    "from transfer_utils import (\n",
    "    load_pretrained_without_ridge,\n",
    "    freeze_layers,\n",
    "    get_trainable_params,\n",
    "    print_parameter_summary,\n",
    ")\n",
    "\n",
    "# „É¢„Éá„É´‰ΩúÊàê\n",
    "model = create_algonauts_model(\n",
    "    subjects=[SUBJECT],\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    seq_len=1,\n",
    "    n_blocks=4,\n",
    "    use_prior=USE_PRIOR,\n",
    "    blurry_recon=BLURRY_RECON,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Model created!\")\n",
    "print_parameter_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f86db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb1f86db",
    "outputId": "78153e41-a0f7-4182-8e84-8eb0075839b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained checkpoint specified. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Ëª¢ÁßªÂ≠¶ÁøíÔºàÊó¢Â≠òckpt„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ\n",
    "if PRETRAINED_CKPT and os.path.exists(PRETRAINED_CKPT):\n",
    "    print(f\"Loading pretrained weights from: {PRETRAINED_CKPT}\")\n",
    "    loaded, missing = load_pretrained_without_ridge(\n",
    "        model,\n",
    "        PRETRAINED_CKPT,\n",
    "        freeze_backbone=True,\n",
    "        freeze_prior=True,\n",
    "    )\n",
    "    print(f\"\\nAfter transfer learning:\")\n",
    "    print_parameter_summary(model)\n",
    "else:\n",
    "    print(\"No pretrained checkpoint specified. Training from scratch.\")\n",
    "    # „Çπ„ÇØ„É©„ÉÉ„ÉÅÂ≠¶Áøí„ÅÆÂ†¥Âêà„ÅØbackbone„ÇÇfreeze„Åó„Å™„ÅÑ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3liT9FTmWvmm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3liT9FTmWvmm",
    "outputId": "8cf66267-5178-474a-fa51-d5533e57a4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä Memory Usage AFTER Model Creation\n",
      "============================================================\n",
      "  cpu_mb: 4652.56 MB\n",
      "  gpu_allocated_mb: 490.02 MB\n",
      "  gpu_reserved_mb: 504.00 MB\n",
      "\n",
      "üìà Memory Increase:\n",
      "  cpu_mb: +4137.39 MB\n",
      "  gpu_allocated_mb: +490.02 MB\n",
      "  gpu_reserved_mb: +504.00 MB\n",
      "\n",
      "üßÆ Theoretical Model Size:\n",
      "  Total params: 128,452,504\n",
      "  Trainable params: 128,452,504\n",
      "  FP32 size: 490.01 MB\n",
      "  FP16 size: 245.00 MB\n",
      "\n",
      "üéØ Model Device: cuda:0\n",
      "   Data Type: torch.float32\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üîç „É°„É¢„É™„Éá„Éê„ÉÉ„Ç∞ - „É¢„Éá„É´‰ΩúÊàêÂæå\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"üìä Memory Usage AFTER Model Creation\")\n",
    "print(\"=\"*60)\n",
    "mem_after = get_memory_info()\n",
    "for k, v in mem_after.items():\n",
    "    print(f\"  {k}: {v:.2f} MB\")\n",
    "\n",
    "print(\"\\nüìà Memory Increase:\")\n",
    "for k in mem_after.keys():\n",
    "    increase = mem_after[k] - mem_before.get(k, 0)\n",
    "    print(f\"  {k}: +{increase:.2f} MB\")\n",
    "\n",
    "# „É¢„Éá„É´„Éë„É©„É°„Éº„ÇøÊï∞\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ÁêÜË´ñÁöÑ„Å™„É°„É¢„É™‰ΩøÁî®ÈáèÔºàFP32ÊÉ≥ÂÆöÔºâ\n",
    "theoretical_mb = total_params * 4 / 1024**2\n",
    "print(f\"\\nüßÆ Theoretical Model Size:\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  FP32 size: {theoretical_mb:.2f} MB\")\n",
    "print(f\"  FP16 size: {theoretical_mb/2:.2f} MB\")\n",
    "\n",
    "# „É¢„Éá„É´„ÅåÂÆüÈöõ„Å´GPU‰∏ä„Å´„ÅÇ„Çã„ÅãÁ¢∫Ë™ç\n",
    "if torch.cuda.is_available():\n",
    "    sample_param = next(model.parameters())\n",
    "    print(f\"\\nüéØ Model Device: {sample_param.device}\")\n",
    "    print(f\"   Data Type: {sample_param.dtype}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d8f04",
   "metadata": {
    "id": "7b3d8f04"
   },
   "source": [
    "## 5. CLIPÁâπÂæ¥„ÅÆÊ∫ñÂÇô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nAEaVivHYaP-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "nAEaVivHYaP-",
    "outputId": "fef4dd00-43bc-4985-87c2-9dc6ed1eb89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DummyCLIPImageEmbedder Memory Check\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip_embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-556445613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# „Éë„É©„É°„Éº„ÇøÊï∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclip_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclip_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mclip_size_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_params\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;31m# FP32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üîç CLIP Embedder „É°„É¢„É™„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "# =============================================================================\n",
    "if DUMMY_MODE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä DummyCLIPImageEmbedder Memory Check\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # „Éë„É©„É°„Éº„ÇøÊï∞\n",
    "    clip_params = sum(p.numel() for p in clip_embedder.parameters())\n",
    "    clip_size_mb = clip_params * 4 / 1024**2  # FP32\n",
    "\n",
    "    print(f\"  Total params: {clip_params:,}\")\n",
    "    print(f\"  Theoretical size (FP32): {clip_size_mb:.2f} MB\")\n",
    "\n",
    "    # „Éá„Éê„Ç§„ÇπÁ¢∫Ë™ç\n",
    "    sample_param = next(clip_embedder.parameters())\n",
    "    print(f\"  Device: {sample_param.device}\")\n",
    "    print(f\"  Dtype: {sample_param.dtype}\")\n",
    "\n",
    "    # „É°„É¢„É™‰ΩøÁî®Èáè\n",
    "    mem_info = get_memory_info()\n",
    "    print(f\"\\n  Current CPU RAM: {mem_info['cpu_mb']:.2f} MB\")\n",
    "    if 'gpu_allocated_mb' in mem_info:\n",
    "        print(f\"  Current GPU RAM: {mem_info['gpu_allocated_mb']:.2f} MB\")\n",
    "\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0f201",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "bc9b5a8cbd3a4119941e9e7b93455500",
      "56d8754cfee447a8a4bbe1c95a6e4ee1",
      "6e5ec983c10741ee87e5fefd371cc748",
      "7996b0610b624a289add344d00a72d9f",
      "650bdbc5a5d744039bf6f7c9aedf59c6",
      "24507560358f462cb8021f8f4ecb42ba",
      "ca7a99b3f5fa4c17b1edadd0f0321314",
      "1369be7e088f48c38298e34d674dd052",
      "2d3f8231f66a4d83b53e90ff2c6a9363",
      "2067b4ce007648cd93ebfa662e873155",
      "37947142af5443ef9a8c764cb956e90c"
     ]
    },
    "id": "f0b0f201",
    "outputId": "4acad06e-62f4-4054-e859-6d159aa77a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenCLIP ViT-B-32 (lightweight version for Colab)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9b5a8cbd3a4119941e9e7b93455500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP loaded! Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# „ÉÄ„Éü„Éº„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØ„ÉÄ„Éü„ÉºCLIP„Çí‰ΩøÁî®\n",
    "if DUMMY_MODE:\n",
    "    from dummy_models import DummyCLIPImageEmbedder\n",
    "\n",
    "    clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)\n",
    "    print(\"Using DummyCLIPImageEmbedder (ËªΩÈáè„ÉÄ„Éü„Éº„É¢„Éá„É´)\")\n",
    "\n",
    "else:\n",
    "    # Êú¨Áâ©„ÅÆCLIP„Çí‰ΩøÁî®ÔºàËªΩÈáèÁâàÔºâ\n",
    "    # ViT-bigG-14„ÅØ5GB‰ª•‰∏ä„ÅßColabÁÑ°ÊñôÁâà„Åß„ÅØ‰Ωø„Åà„Å™„ÅÑ„Åü„ÇÅ„ÄÅËªΩÈáèÁâà„Çí‰ΩøÁî®\n",
    "    try:\n",
    "        import open_clip\n",
    "\n",
    "        # ËªΩÈáèÁâàCLIPÔºàViT-B/32: Á¥Ñ350MBÔºâ\n",
    "        print(\"Loading OpenCLIP ViT-B-32 (lightweight version for Colab)...\")\n",
    "        clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            \"ViT-B-32\",  # bigG-14„ÅÆ‰ª£„Çè„Çä„Å´ËªΩÈáèÁâà„Çí‰ΩøÁî®\n",
    "            pretrained=\"laion2b_s34b_b79k\",\n",
    "        )\n",
    "        clip_model = clip_model.to(DEVICE).eval()\n",
    "\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(f\"OpenCLIP loaded! Model on device: {next(clip_model.parameters()).device}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load OpenCLIP: {e}\")\n",
    "        print(\"Falling back to dummy mode\")\n",
    "        DUMMY_MODE = True\n",
    "        from dummy_models import DummyCLIPImageEmbedder\n",
    "        clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "412dc816",
    "outputId": "f3460962-0ab6-419f-b759-a4af4063dfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP features shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "def get_clip_features(images):\n",
    "    \"\"\"ÁîªÂÉè„Åã„ÇâCLIPÁâπÂæ¥„ÇíÊäΩÂá∫\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if DUMMY_MODE:\n",
    "            return clip_embedder(images)\n",
    "        else:\n",
    "            # OpenCLIP„Çí‰ΩøÁî®\n",
    "            features = clip_model.encode_image(images)\n",
    "            return features\n",
    "\n",
    "# „ÉÜ„Çπ„Éà\n",
    "test_images = sample_batch['image'].to(DEVICE)\n",
    "test_features = get_clip_features(test_images)\n",
    "print(f\"CLIP features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e701814",
   "metadata": {
    "id": "8e701814"
   },
   "source": [
    "## 6. Â≠¶Áøí„É´„Éº„Éó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d53128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6d53128",
    "outputId": "2e716f83-9cc4-43b8-e99b-bc321b559e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 128,452,504\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂ÔºàÂ≠¶ÁøíÂèØËÉΩ„Å™„Éë„É©„É°„Éº„Çø„ÅÆ„ÅøÔºâ\n",
    "trainable_params = get_trainable_params(model, mode=\"all_unfrozen\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "optimizer = AdamW(trainable_params, lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "# „Çπ„Ç±„Ç∏„É•„Éº„É©\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06b396",
   "metadata": {
    "id": "0d06b396"
   },
   "outputs": [],
   "source": [
    "def soft_clip_loss(preds, targets, temp=0.006):\n",
    "    \"\"\"\n",
    "    Soft CLIP contrastive loss (ÂÖÉ„ÅÆutils.py„Å®Âêå„Åò„É≠„Ç∏„ÉÉ„ÇØ)\n",
    "    \n",
    "    Args:\n",
    "        preds: (batch, emb_dim) - „É¢„Éá„É´‰∫àÊ∏¨Ôºàbrain ‚Üí CLIPÔºâ\n",
    "        targets: (batch, emb_dim) - ÂÆüÈöõ„ÅÆCLIPÁâπÂæ¥\n",
    "        temp: temperature parameter\n",
    "    \"\"\"\n",
    "    # preds/targets„ÅØ2Ê¨°ÂÖÉ (batch, emb_dim) „ÇíÊÉ≥ÂÆö\n",
    "    # „ÇÇ„Åó3Ê¨°ÂÖÉ„Å™„ÇâÊúÄÂæå„ÅÆÊ¨°ÂÖÉ‰ª•Â§ñ„Çíflatten\n",
    "    if preds.dim() == 3:\n",
    "        preds = preds.reshape(preds.shape[0], -1)\n",
    "    if targets.dim() == 3:\n",
    "        targets = targets.reshape(targets.shape[0], -1)\n",
    "    \n",
    "    # Ê¨°ÂÖÉ„Åå‰∏ÄËá¥„Åó„Å™„ÅÑÂ†¥Âêà„ÅØpreds„ÇíÂàá„ÇäË©∞„ÇÅ„ÇãÔºàÁ∑äÊÄ•ÂØæÂøúÔºâ\n",
    "    if preds.shape[1] != targets.shape[1]:\n",
    "        print(f\"Warning: dimension mismatch - preds:{preds.shape[1]}, targets:{targets.shape[1]}\")\n",
    "        # CLIP„ÅÆÊ¨°ÂÖÉ„Å´Âêà„Çè„Åõ„Å¶Á∑öÂΩ¢Â§âÊèõÔºàÂ≠¶ÁøíÂèØËÉΩ„Å´„Åô„ÇãÂ†¥Âêà„ÅØË¶ÅÊ§úË®éÔºâ\n",
    "        if not hasattr(soft_clip_loss, 'dim_adapter'):\n",
    "            soft_clip_loss.dim_adapter = nn.Linear(preds.shape[1], targets.shape[1]).to(preds.device)\n",
    "        preds = soft_clip_loss.dim_adapter(preds)\n",
    "    \n",
    "    # ÂÖÉ„ÅÆutils.py„Å®Âêå„Åò„É≠„Ç∏„ÉÉ„ÇØ\n",
    "    clip_clip = (targets @ targets.T) / temp\n",
    "    brain_clip = (preds @ targets.T) / temp\n",
    "    \n",
    "    loss1 = -(brain_clip.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "    loss2 = -(brain_clip.T.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n",
    "    \n",
    "    return (loss1 + loss2) / 2\n",
    "\n",
    "def train_step(batch):\n",
    "    \"\"\"1„Éê„ÉÉ„ÉÅ„ÅÆÂ≠¶Áøí„Çπ„ÉÜ„ÉÉ„Éó\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # „Éá„Éº„ÇøÂèñÂæó\n",
    "    fmri = batch['fmri'].to(DEVICE)\n",
    "    images = batch['image'].to(DEVICE)\n",
    "\n",
    "    # CLIPÁâπÂæ¥„ÇíÂèñÂæóÔºà„Çø„Éº„Ç≤„ÉÉ„ÉàÔºâ\n",
    "    with torch.no_grad():\n",
    "        clip_target = get_clip_features(images)  # (batch, emb_dim)\n",
    "\n",
    "    # Forward\n",
    "    backbone, clip_voxels, blurry = model(fmri)\n",
    "\n",
    "    # ÂΩ¢Áä∂„Çí„Éá„Éê„ÉÉ„Ç∞Âá∫ÂäõÔºàÊúÄÂàù„ÅÆ„Éê„ÉÉ„ÉÅ„ÅÆ„ÅøÔºâ\n",
    "    if not hasattr(train_step, 'debug_printed'):\n",
    "        print(f\"Debug shapes:\")\n",
    "        print(f\"  fmri: {fmri.shape}\")\n",
    "        print(f\"  backbone: {backbone.shape}\")\n",
    "        print(f\"  clip_voxels: {clip_voxels.shape}\")\n",
    "        print(f\"  clip_target: {clip_target.shape}\")\n",
    "        train_step.debug_printed = True\n",
    "\n",
    "    # clip_voxels„Çíflatten„Åó„Å¶‰Ωø„ÅÜ\n",
    "    # clip_voxels: (batch, seq_len, clip_emb_dim) -> (batch, seq_len * clip_emb_dim)\n",
    "    clip_voxels_flat = clip_voxels.reshape(clip_voxels.shape[0], -1)\n",
    "    clip_target_flat = clip_target.reshape(clip_target.shape[0], -1)\n",
    "\n",
    "    # LossË®àÁÆó\n",
    "    loss = soft_clip_loss(clip_voxels_flat, clip_target_flat)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d188f2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d188f2f",
    "outputId": "52fa9357-6fd9-4259-da49-5b7ccb10e196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 1.06 GB allocated\n",
      "GPU Memory: 1.10 GB reserved\n"
     ]
    }
   ],
   "source": [
    "# „É°„É¢„É™Á¢∫Ë™ç\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0367f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "d0dfe203d23041b9aeede9f0e54bd3e7",
      "69e934c6520a4fd0a724de7df1039aa6",
      "eb163e2004d8414c93af1e6961ce037a",
      "cd7340b752ea407cbc71ff3c8c05c0f2",
      "394d268177c7432db1d1f2c98b2990d8",
      "285ba4c598834b5b8b35babe0cfa09ce",
      "18fca8fde273464bbebfa68a15c12147",
      "42814c33b28d4295a0658c9985f5eb88",
      "9dea039f804742099391c34f85938ec6",
      "ebb8a67534a84a0cb2a244dde0e787fb",
      "e12e2de99cc44400a2728ef893ddef6d"
     ]
    },
    "id": "51e0367f",
    "outputId": "57cb5dbf-3154-4867-992a-9f0acb0f7d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting training: 1 epochs, 4920 batches/epoch\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dfe203d23041b9aeede9f0e54bd3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/4920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x425984 and 512x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2470480545.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{loss:.4f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-639488239.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mclip_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_clip_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_voxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-639488239.py\u001b[0m in \u001b[0;36msoft_clip_loss\u001b[0;34m(preds, targets, temp)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x425984 and 512x2)"
     ]
    }
   ],
   "source": [
    "# Â≠¶Áøí„É´„Éº„Éó\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_loader)} batches/epoch\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        loss = train_step(batch)\n",
    "        epoch_losses.append(loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # „É°„É¢„É™Á¢∫Ë™ç\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf52af",
   "metadata": {
    "id": "f1cf52af"
   },
   "outputs": [],
   "source": [
    "# ÊêçÂ§±„ÅÆÂèØË¶ñÂåñ\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e4d2f",
   "metadata": {
    "id": "f70e4d2f"
   },
   "source": [
    "## 7. „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b93a67",
   "metadata": {
    "id": "39b93a67"
   },
   "outputs": [],
   "source": [
    "from transfer_utils import save_checkpoint\n",
    "\n",
    "# ‰øùÂ≠òÂÖà„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê\n",
    "save_dir = os.path.join(CHECKPOINT_DIR, f\"algonauts_{SUBJECT}_{TRAIN_MODE}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà‰øùÂ≠ò\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=NUM_EPOCHS,\n",
    "    save_path=os.path.join(save_dir, \"last.pth\"),\n",
    "    extra_info={\n",
    "        \"train_mode\": TRAIN_MODE,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"final_loss\": losses[-1] if losses else None,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"\\nCheckpoint saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c46259",
   "metadata": {
    "id": "d2c46259"
   },
   "source": [
    "## 8. Á∞°ÊòìÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dee4e2",
   "metadata": {
    "id": "38dee4e2"
   },
   "outputs": [],
   "source": [
    "# Êé®Ë´ñ„ÉÜ„Çπ„Éà\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_fmri = test_batch['fmri'].to(DEVICE)\n",
    "    test_images = test_batch['image'].to(DEVICE)\n",
    "\n",
    "    # fMRI ‚Üí CLIP tokens\n",
    "    backbone, clip_voxels, blurry = model(test_fmri)\n",
    "\n",
    "    # ÂÆüÈöõ„ÅÆCLIPÁâπÂæ¥\n",
    "    clip_target = get_clip_features(test_images)\n",
    "    if clip_target.dim() == 2:\n",
    "        clip_target = clip_target.unsqueeze(1)\n",
    "\n",
    "    # „Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶\n",
    "    pred_flat = F.normalize(clip_voxels.view(clip_voxels.shape[0], -1), dim=-1)\n",
    "    target_flat = F.normalize(clip_target.view(clip_target.shape[0], -1), dim=-1)\n",
    "\n",
    "    similarity = (pred_flat * target_flat).sum(dim=-1).mean()\n",
    "\n",
    "    print(f\"\\nInference test:\")\n",
    "    print(f\"  Input fMRI shape: {test_fmri.shape}\")\n",
    "    print(f\"  Output CLIP shape: {clip_voxels.shape}\")\n",
    "    print(f\"  Average cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962d6be",
   "metadata": {
    "id": "d962d6be"
   },
   "outputs": [],
   "source": [
    "# ÂÖ•ÂäõÁîªÂÉè„ÅÆÂèØË¶ñÂåñ\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"ImageNetÊ≠£Ë¶èÂåñ„ÇíÂÖÉ„Å´Êàª„Åô\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# „Çµ„É≥„Éó„É´ÁîªÂÉè„ÇíË°®Á§∫\n",
    "sample_images = denormalize(test_images[:4])\n",
    "grid = make_grid(sample_images, nrow=4).cpu().permute(1, 2, 0).numpy()\n",
    "grid = np.clip(grid, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(grid)\n",
    "plt.title(\"Sample Training Images\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5bbd3",
   "metadata": {
    "id": "f9d5bbd3"
   },
   "source": [
    "## Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó\n",
    "\n",
    "1. **„ÉÄ„Éü„Éº„É¢„Éº„Éâ„ÅßÂãï‰ΩúÁ¢∫Ë™ç** ‚Üí „Ç®„É©„Éº„Å™„ÅèÂÆå‰∫Ü„Åô„Çå„Å∞OK\n",
    "2. **ËªΩÈáè„É¢„Éº„ÉâÔºàlightÔºâ„ÅßÂÆüÂ≠¶Áøí** ‚Üí T4„ÅßÊï∞ÊôÇÈñì\n",
    "3. **Ê®ôÊ∫ñ„É¢„Éº„ÉâÔºàstandardÔºâ„ÅßÊú¨Ê†ºÂ≠¶Áøí** ‚Üí Pro or Á†îÁ©∂ÂÆ§PC\n",
    "4. **Êé®Ë´ñ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ** ‚Üí `mindeye_inference_colab.ipynb` „ÅßÁîªÂÉèÂÜçÊßãÊàê"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fmri3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1369be7e088f48c38298e34d674dd052": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18fca8fde273464bbebfa68a15c12147": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2067b4ce007648cd93ebfa662e873155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24507560358f462cb8021f8f4ecb42ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285ba4c598834b5b8b35babe0cfa09ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d3f8231f66a4d83b53e90ff2c6a9363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37947142af5443ef9a8c764cb956e90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "394d268177c7432db1d1f2c98b2990d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42814c33b28d4295a0658c9985f5eb88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56d8754cfee447a8a4bbe1c95a6e4ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24507560358f462cb8021f8f4ecb42ba",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ca7a99b3f5fa4c17b1edadd0f0321314",
      "value": "open_clip_model.safetensors:‚Äá100%"
     }
    },
    "650bdbc5a5d744039bf6f7c9aedf59c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69e934c6520a4fd0a724de7df1039aa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_285ba4c598834b5b8b35babe0cfa09ce",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_18fca8fde273464bbebfa68a15c12147",
      "value": "Epoch‚Äá1/1:‚Äá‚Äá‚Äá0%"
     }
    },
    "6e5ec983c10741ee87e5fefd371cc748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1369be7e088f48c38298e34d674dd052",
      "max": 605143316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d3f8231f66a4d83b53e90ff2c6a9363",
      "value": 605143316
     }
    },
    "7996b0610b624a289add344d00a72d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2067b4ce007648cd93ebfa662e873155",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_37947142af5443ef9a8c764cb956e90c",
      "value": "‚Äá605M/605M‚Äá[00:06&lt;00:00,‚Äá178MB/s]"
     }
    },
    "9dea039f804742099391c34f85938ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bc9b5a8cbd3a4119941e9e7b93455500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56d8754cfee447a8a4bbe1c95a6e4ee1",
       "IPY_MODEL_6e5ec983c10741ee87e5fefd371cc748",
       "IPY_MODEL_7996b0610b624a289add344d00a72d9f"
      ],
      "layout": "IPY_MODEL_650bdbc5a5d744039bf6f7c9aedf59c6"
     }
    },
    "ca7a99b3f5fa4c17b1edadd0f0321314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd7340b752ea407cbc71ff3c8c05c0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebb8a67534a84a0cb2a244dde0e787fb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e12e2de99cc44400a2728ef893ddef6d",
      "value": "‚Äá0/4920‚Äá[00:01&lt;?,‚Äá?it/s]"
     }
    },
    "d0dfe203d23041b9aeede9f0e54bd3e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69e934c6520a4fd0a724de7df1039aa6",
       "IPY_MODEL_eb163e2004d8414c93af1e6961ce037a",
       "IPY_MODEL_cd7340b752ea407cbc71ff3c8c05c0f2"
      ],
      "layout": "IPY_MODEL_394d268177c7432db1d1f2c98b2990d8"
     }
    },
    "e12e2de99cc44400a2728ef893ddef6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb163e2004d8414c93af1e6961ce037a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42814c33b28d4295a0658c9985f5eb88",
      "max": 4920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9dea039f804742099391c34f85938ec6",
      "value": 0
     }
    },
    "ebb8a67534a84a0cb2a244dde0e787fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30e04e0d",
      "metadata": {
        "id": "30e04e0d"
      },
      "source": [
        "## 1. 設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9f18df81",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f18df81",
        "outputId": "684115ea-1751-451b-f8cf-ba1412f04e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting /content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip to /content/data/algonauts_2023_challenge_data/train_data/\n",
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Google Driveをマウント（既にマウント済みの場合はスキップ）\n",
        "\n",
        "# マウント\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# zipファイルのパス（例：Google Drive上のアップロード先）\n",
        "zip_path = \"/content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip\"  # 変更してください\n",
        "\n",
        "# 展開先ディレクトリ\n",
        "extract_path = \"/content/data/algonauts_2023_challenge_data/train_data/\"  # 変更してください\n",
        "\n",
        "# zipファイルの確認と展開\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Extracting {zip_path} to {extract_path}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete!\")\n",
        "else:\n",
        "    print(f\"Zip file not found at: {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1de1933e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de1933e",
        "outputId": "7e6a5c44-3815-460b-c2a5-a67ef5e8bb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: dummy\n",
            "Hidden Dim: 256\n",
            "Batch Size: 2\n",
            "Epochs: 1\n",
            "Dummy Mode: True\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 学習設定 - 必要に応じて変更してください\n",
        "# =============================================================================\n",
        "\n",
        "# 学習モード: \"dummy\" | \"light\" | \"standard\"\n",
        "TRAIN_MODE = \"dummy\"\n",
        "\n",
        "# 被験者ID\n",
        "SUBJECT = \"subj01\"\n",
        "\n",
        "# データパス（Google Drive上）\n",
        "DATA_ROOT = \"/content/data/algonauts_2023_challenge_data/train_data/\"\n",
        "\n",
        "# チェックポイント保存先\n",
        "CHECKPOINT_DIR = \"/content/data/mindeye_checkpoints\"\n",
        "\n",
        "# 既存の学習済みモデル（転移学習用、Noneの場合はスキップ）\n",
        "PRETRAINED_CKPT = None  # 例: \"/content/drive/MyDrive/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
        "\n",
        "# =============================================================================\n",
        "# モード別設定（自動設定）\n",
        "# =============================================================================\n",
        "if TRAIN_MODE == \"dummy\":\n",
        "    HIDDEN_DIM = 256\n",
        "    BATCH_SIZE = 2\n",
        "    NUM_EPOCHS = 1\n",
        "    USE_PRIOR = False\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = True\n",
        "elif TRAIN_MODE == \"light\":\n",
        "    HIDDEN_DIM = 512\n",
        "    BATCH_SIZE = 4\n",
        "    NUM_EPOCHS = 10\n",
        "    USE_PRIOR = False\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = False\n",
        "else:  # standard\n",
        "    HIDDEN_DIM = 1024\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_EPOCHS = 50\n",
        "    USE_PRIOR = True\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = False\n",
        "\n",
        "print(f\"Mode: {TRAIN_MODE}\")\n",
        "print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Dummy Mode: {DUMMY_MODE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f892613",
      "metadata": {
        "id": "9f892613"
      },
      "source": [
        "## 2. 環境構築"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fef90d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fef90d8a",
        "outputId": "d6bd5fc5-2ec3-4dfd-c267-d3a4f57d721d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 13 02:15:55 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPUの確認\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "24a979c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24a979c8",
        "outputId": "b5bc23bf-a826-4b41-9a5f-7baa44879a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 依存パッケージのインストール\n",
        "!pip install -q torch torchvision einops omegaconf h5py tqdm accelerate\n",
        "\n",
        "# ダミーモードでない場合は追加パッケージをインストール\n",
        "if not DUMMY_MODE:\n",
        "    !pip install -q open_clip_torch diffusers transformers kornia webdataset dalle2_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1062b8c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1062b8c4",
        "outputId": "7540a923-e273-49cb-93f8-0d664871652b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/MindEyeV2'...\n",
            "remote: Enumerating objects: 668, done.\u001b[K\n",
            "remote: Counting objects: 100% (315/315), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 668 (delta 231), reused 212 (delta 200), pack-reused 353 (from 1)\u001b[K\n",
            "Receiving objects: 100% (668/668), 1.11 GiB | 17.97 MiB/s, done.\n",
            "Resolving deltas: 100% (344/344), done.\n",
            "Updating files: 100% (155/155), done.\n",
            "Working directory: /content/MindEyeV2\n"
          ]
        }
      ],
      "source": [
        "# リポジトリのクローン（既存の場合はスキップ）\n",
        "import os\n",
        "if not os.path.exists(\"/content/MindEyeV2\"):\n",
        "    # TODO: 自分のフォークしたリポジトリURLに変更してください\n",
        "    !git clone https://github.com/boxed-mikann/MindEyeV2.git /content/MindEyeV2\n",
        "else:\n",
        "    print(\"Repository already exists\")\n",
        "\n",
        "# パスを追加\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
        "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
        "\n",
        "os.chdir(\"/content/MindEyeV2\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pulling latest changes from MindEyeV2 repository...')\n",
        "!git pull\n",
        "print('Repository update complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StGS0JkILkyW",
        "outputId": "63352860-219c-4b7f-fdbf-f4a4a2577392"
      },
      "id": "StGS0JkILkyW",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling latest changes from MindEyeV2 repository...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 1022 bytes | 204.00 KiB/s, done.\n",
            "From https://github.com/boxed-mikann/MindEyeV2\n",
            "   17c949f..a1027a6  main       -> origin/main\n",
            "Updating 17c949f..a1027a6\n",
            "Fast-forward\n",
            " mysrc/__init__.py          | 80 \u001b[32m+++++++++++++++++++++++++++++++\u001b[m\u001b[31m---------------\u001b[m\n",
            " mysrc/algonauts_dataset.py |  5 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " mysrc/models_algonauts.py  | 11 \u001b[32m+++++\u001b[m\u001b[31m--\u001b[m\n",
            " mysrc/transfer_utils.py    |  5 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " 4 files changed, 72 insertions(+), 29 deletions(-)\n",
            "Repository update complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6985daca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6985daca",
        "outputId": "a75afcaa-e5d9-47db-84df-7584483cdd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Data found: /content/data/algonauts_2023_challenge_data/train_data/subj01\n",
            "total 20\n",
            "drwxr-xr-x 5 root root 4096 Jan 13 02:13 .\n",
            "drwxr-xr-x 3 root root 4096 Jan 13 02:13 ..\n",
            "drwxr-xr-x 2 root root 4096 Jan 13 02:13 roi_masks\n",
            "drwxr-xr-x 3 root root 4096 Jan 13 02:13 test_split\n",
            "drwxr-xr-x 4 root root 4096 Jan 13 02:14 training_split\n"
          ]
        }
      ],
      "source": [
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# データの存在確認\n",
        "import os\n",
        "subj_dir = os.path.join(DATA_ROOT, SUBJECT)\n",
        "if os.path.exists(subj_dir):\n",
        "    print(f\"✓ Data found: {subj_dir}\")\n",
        "    !ls -la {subj_dir}\n",
        "else:\n",
        "    print(f\"✗ Data NOT found: {subj_dir}\")\n",
        "    print(\"Please upload Algonauts2023 data to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf51bdb",
      "metadata": {
        "id": "2bf51bdb"
      },
      "source": [
        "## 3. データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3f3bafa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f3bafa2",
        "outputId": "5db5d7b4-3a12-47ca-b97d-a6cc55529d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MindEyeV2 Algonauts Configuration\n",
            "============================================================\n",
            "Environment:     Colab\n",
            "Device:          cuda\n",
            "Dummy Mode:      True\n",
            "Data Root:       /content/drive/MyDrive/algonauts_2023_challenge_data\n",
            "Checkpoint Dir:  /content/drive/MyDrive/mindeye_checkpoints\n",
            "Output Dir:      /content/outputs\n",
            "SRC Dir:         /content/MindEyeV2/src\n",
            "============================================================\n",
            "\n",
            "Device: cuda\n",
            "Total vertices for subj01: 39548\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 環境変数でダミーモードを設定\n",
        "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
        "\n",
        "# mysrcモジュールをインポート\n",
        "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
        "from config import print_config, DEVICE\n",
        "\n",
        "print_config()\n",
        "print(f\"\\nDevice: {DEVICE}\")\n",
        "print(f\"Total vertices for {SUBJECT}: {get_total_vertices(SUBJECT)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "936f9ce3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "936f9ce3",
        "outputId": "940be68b-cabd-4aaa-f04c-906bb4542d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fMRI data for subj01...\n",
            "  LH shape: (9841, 19004), RH shape: (9841, 20544)\n",
            "  Combined shape: (9841, 39548)\n",
            "Found 9841 train images\n",
            "\n",
            "Dataset size: 9841\n",
            "Number of batches: 4920\n",
            "\n",
            "Sample batch:\n",
            "  fMRI shape: torch.Size([2, 39548])\n",
            "  Image shape: torch.Size([2, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# データローダー作成\n",
        "train_loader = get_dataloader(\n",
        "    data_root=DATA_ROOT,\n",
        "    subject=SUBJECT,\n",
        "    split=\"train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset size: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of batches: {len(train_loader)}\")\n",
        "\n",
        "# サンプルバッチを取得\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch:\")\n",
        "print(f\"  fMRI shape: {sample_batch['fmri'].shape}\")\n",
        "print(f\"  Image shape: {sample_batch['image'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3882cc",
      "metadata": {
        "id": "6e3882cc"
      },
      "source": [
        "## 4. モデル作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0f9d2104",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "0f9d2104",
        "outputId": "cd57e531-a0c6-4dc7-cb56-2a1a10e2fff6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: No module named 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/content/MindEyeV2/mysrc/models_algonauts.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBrainNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             self.backbone = BrainNetwork(\n",
            "\u001b[0;32m/content/MindEyeV2/src/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-110482904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# モデル作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model = create_algonauts_model(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msubjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSUBJECT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MindEyeV2/mysrc/models_algonauts.py\u001b[0m in \u001b[0;36mcreate_algonauts_model\u001b[0;34m(subjects, hidden_dim, seq_len, n_blocks, use_prior, blurry_recon, device)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mAlgonautsMindEye\u001b[0m \u001b[0mインスタンス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m     model = AlgonautsMindEye(\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0msubjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/MindEyeV2/mysrc/models_algonauts.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, subjects, hidden_dim, out_dim, seq_len, n_blocks, clip_emb_dim, clip_seq_dim, use_prior, blurry_recon, clip_scale, drop)\u001b[0m\n\u001b[1;32m    179\u001b[0m             )\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;34mf\"Could not import BrainNetwork from src/models.py. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;34mf\"Make sure src/ is in your Python path. Error: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import BrainNetwork from src/models.py. Make sure src/ is in your Python path. Error: No module named 'clip'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from models_algonauts import AlgonautsMindEye, create_algonauts_model\n",
        "from transfer_utils import (\n",
        "    load_pretrained_without_ridge,\n",
        "    freeze_layers,\n",
        "    get_trainable_params,\n",
        "    print_parameter_summary,\n",
        ")\n",
        "\n",
        "# モデル作成\n",
        "model = create_algonauts_model(\n",
        "    subjects=[SUBJECT],\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    seq_len=1,\n",
        "    n_blocks=4,\n",
        "    use_prior=USE_PRIOR,\n",
        "    blurry_recon=BLURRY_RECON,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "print(\"Model created!\")\n",
        "print_parameter_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1f86db",
      "metadata": {
        "id": "cb1f86db"
      },
      "outputs": [],
      "source": [
        "# 転移学習（既存ckptがある場合）\n",
        "if PRETRAINED_CKPT and os.path.exists(PRETRAINED_CKPT):\n",
        "    print(f\"Loading pretrained weights from: {PRETRAINED_CKPT}\")\n",
        "    loaded, missing = load_pretrained_without_ridge(\n",
        "        model,\n",
        "        PRETRAINED_CKPT,\n",
        "        freeze_backbone=True,\n",
        "        freeze_prior=True,\n",
        "    )\n",
        "    print(f\"\\nAfter transfer learning:\")\n",
        "    print_parameter_summary(model)\n",
        "else:\n",
        "    print(\"No pretrained checkpoint specified. Training from scratch.\")\n",
        "    # スクラッチ学習の場合はbackboneもfreezeしない\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3d8f04",
      "metadata": {
        "id": "7b3d8f04"
      },
      "source": [
        "## 5. CLIP特徴の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b0f201",
      "metadata": {
        "id": "f0b0f201"
      },
      "outputs": [],
      "source": [
        "# ダミーモードの場合はダミーCLIPを使用\n",
        "if DUMMY_MODE:\n",
        "    from dummy_models import DummyCLIPImageEmbedder, get_dummy_clip_features\n",
        "\n",
        "    clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)\n",
        "    print(\"Using DummyCLIPImageEmbedder\")\n",
        "else:\n",
        "    # 本物のCLIPを使用\n",
        "    try:\n",
        "        import open_clip\n",
        "\n",
        "        # ViT-bigG-14 のロード（重い）\n",
        "        print(\"Loading OpenCLIP ViT-bigG-14... (this may take a while)\")\n",
        "        clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "            \"ViT-bigG-14\",\n",
        "            pretrained=\"laion2b_s39b_b160k\",\n",
        "        )\n",
        "        clip_model = clip_model.to(DEVICE).eval()\n",
        "\n",
        "        for param in clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(\"OpenCLIP loaded!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load OpenCLIP: {e}\")\n",
        "        print(\"Falling back to dummy mode\")\n",
        "        DUMMY_MODE = True\n",
        "        from dummy_models import DummyCLIPImageEmbedder\n",
        "        clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412dc816",
      "metadata": {
        "id": "412dc816"
      },
      "outputs": [],
      "source": [
        "def get_clip_features(images):\n",
        "    \"\"\"画像からCLIP特徴を抽出\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if DUMMY_MODE:\n",
        "            return clip_embedder(images)\n",
        "        else:\n",
        "            # OpenCLIPを使用\n",
        "            features = clip_model.encode_image(images)\n",
        "            return features\n",
        "\n",
        "# テスト\n",
        "test_images = sample_batch['image'].to(DEVICE)\n",
        "test_features = get_clip_features(test_images)\n",
        "print(f\"CLIP features shape: {test_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e701814",
      "metadata": {
        "id": "8e701814"
      },
      "source": [
        "## 6. 学習ループ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d53128",
      "metadata": {
        "id": "a6d53128"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# オプティマイザ（学習可能なパラメータのみ）\n",
        "trainable_params = get_trainable_params(model, mode=\"all_unfrozen\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "\n",
        "optimizer = AdamW(trainable_params, lr=3e-4, weight_decay=1e-2)\n",
        "\n",
        "# スケジューラ\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=3e-4,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d06b396",
      "metadata": {
        "id": "0d06b396"
      },
      "outputs": [],
      "source": [
        "def soft_clip_loss(preds, targets, temp=0.006):\n",
        "    \"\"\"Soft CLIP contrastive loss\"\"\"\n",
        "    # Flatten to (batch, dim)\n",
        "    preds = preds.view(preds.shape[0], -1)\n",
        "    targets = targets.view(targets.shape[0], -1)\n",
        "\n",
        "    # Normalize\n",
        "    preds = F.normalize(preds, dim=-1)\n",
        "    targets = F.normalize(targets, dim=-1)\n",
        "\n",
        "    # Cosine similarity\n",
        "    logits = (preds @ targets.T) / temp\n",
        "    labels = torch.arange(len(logits), device=logits.device)\n",
        "\n",
        "    loss_i = F.cross_entropy(logits, labels)\n",
        "    loss_t = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    return (loss_i + loss_t) / 2\n",
        "\n",
        "def train_step(batch):\n",
        "    \"\"\"1バッチの学習ステップ\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # データ取得\n",
        "    fmri = batch['fmri'].to(DEVICE)\n",
        "    images = batch['image'].to(DEVICE)\n",
        "\n",
        "    # CLIP特徴を取得（ターゲット）\n",
        "    with torch.no_grad():\n",
        "        clip_target = get_clip_features(images)\n",
        "\n",
        "    # Forward\n",
        "    backbone, clip_voxels, blurry = model(fmri)\n",
        "\n",
        "    # Loss計算\n",
        "    # clip_voxels: (batch, seq, emb_dim)\n",
        "    # clip_target: (batch, seq, emb_dim) or (batch, emb_dim)\n",
        "    if clip_target.dim() == 2:\n",
        "        clip_target = clip_target.unsqueeze(1)\n",
        "\n",
        "    loss = soft_clip_loss(clip_voxels, clip_target)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d188f2f",
      "metadata": {
        "id": "0d188f2f"
      },
      "outputs": [],
      "source": [
        "# メモリ確認\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e0367f",
      "metadata": {
        "id": "51e0367f"
      },
      "outputs": [],
      "source": [
        "# 学習ループ\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_loader)} batches/epoch\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_losses = []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    for batch in pbar:\n",
        "        loss = train_step(batch)\n",
        "        epoch_losses.append(loss)\n",
        "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
        "\n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    losses.extend(epoch_losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # メモリ確認\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training complete!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cf52af",
      "metadata": {
        "id": "f1cf52af"
      },
      "outputs": [],
      "source": [
        "# 損失の可視化\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70e4d2f",
      "metadata": {
        "id": "f70e4d2f"
      },
      "source": [
        "## 7. チェックポイント保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b93a67",
      "metadata": {
        "id": "39b93a67"
      },
      "outputs": [],
      "source": [
        "from transfer_utils import save_checkpoint\n",
        "\n",
        "# 保存先ディレクトリ作成\n",
        "save_dir = os.path.join(CHECKPOINT_DIR, f\"algonauts_{SUBJECT}_{TRAIN_MODE}\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# チェックポイント保存\n",
        "save_checkpoint(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    epoch=NUM_EPOCHS,\n",
        "    save_path=os.path.join(save_dir, \"last.pth\"),\n",
        "    extra_info={\n",
        "        \"train_mode\": TRAIN_MODE,\n",
        "        \"subject\": SUBJECT,\n",
        "        \"hidden_dim\": HIDDEN_DIM,\n",
        "        \"final_loss\": losses[-1] if losses else None,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"\\nCheckpoint saved to: {save_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c46259",
      "metadata": {
        "id": "d2c46259"
      },
      "source": [
        "## 8. 簡易検証"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38dee4e2",
      "metadata": {
        "id": "38dee4e2"
      },
      "outputs": [],
      "source": [
        "# 推論テスト\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_batch = next(iter(train_loader))\n",
        "    test_fmri = test_batch['fmri'].to(DEVICE)\n",
        "    test_images = test_batch['image'].to(DEVICE)\n",
        "\n",
        "    # fMRI → CLIP tokens\n",
        "    backbone, clip_voxels, blurry = model(test_fmri)\n",
        "\n",
        "    # 実際のCLIP特徴\n",
        "    clip_target = get_clip_features(test_images)\n",
        "    if clip_target.dim() == 2:\n",
        "        clip_target = clip_target.unsqueeze(1)\n",
        "\n",
        "    # コサイン類似度\n",
        "    pred_flat = F.normalize(clip_voxels.view(clip_voxels.shape[0], -1), dim=-1)\n",
        "    target_flat = F.normalize(clip_target.view(clip_target.shape[0], -1), dim=-1)\n",
        "\n",
        "    similarity = (pred_flat * target_flat).sum(dim=-1).mean()\n",
        "\n",
        "    print(f\"\\nInference test:\")\n",
        "    print(f\"  Input fMRI shape: {test_fmri.shape}\")\n",
        "    print(f\"  Output CLIP shape: {clip_voxels.shape}\")\n",
        "    print(f\"  Average cosine similarity: {similarity.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d962d6be",
      "metadata": {
        "id": "d962d6be"
      },
      "outputs": [],
      "source": [
        "# 入力画像の可視化\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"ImageNet正規化を元に戻す\"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
        "    return tensor * std + mean\n",
        "\n",
        "# サンプル画像を表示\n",
        "sample_images = denormalize(test_images[:4])\n",
        "grid = make_grid(sample_images, nrow=4).cpu().permute(1, 2, 0).numpy()\n",
        "grid = np.clip(grid, 0, 1)\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.imshow(grid)\n",
        "plt.title(\"Sample Training Images\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d5bbd3",
      "metadata": {
        "id": "f9d5bbd3"
      },
      "source": [
        "## 次のステップ\n",
        "\n",
        "1. **ダミーモードで動作確認** → エラーなく完了すればOK\n",
        "2. **軽量モード（light）で実学習** → T4で数時間\n",
        "3. **標準モード（standard）で本格学習** → Pro or 研究室PC\n",
        "4. **推論ノートブック** → `mindeye_inference_colab.ipynb` で画像再構成"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
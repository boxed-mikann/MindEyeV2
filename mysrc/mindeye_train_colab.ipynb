{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30e04e0d",
      "metadata": {
        "id": "30e04e0d"
      },
      "source": [
        "## 1. è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f18df81",
      "metadata": {
        "vscode": {
          "languageId": "ini"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f18df81",
        "outputId": "0bcb8e47-1372-4486-ab9e-025e47b96652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting /content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip to /content/data/algonauts_2023_challenge_data/train_data/\n",
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆï¼ˆæ—¢ã«ãƒã‚¦ãƒ³ãƒˆæ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "\n",
        "# ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# zipãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹ï¼šGoogle Driveä¸Šã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆï¼‰\n",
        "zip_path = \"/content/drive/MyDrive/algonauts_2023_challenge_data/train_data/subj01.zip\"  # å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "\n",
        "# å±•é–‹å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "extract_path = \"/content/data/algonauts_2023_challenge_data/train_data/\"  # å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "\n",
        "# zipãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨å±•é–‹\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Extracting {zip_path} to {extract_path}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction complete!\")\n",
        "else:\n",
        "    print(f\"Zip file not found at: {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1de1933e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de1933e",
        "outputId": "8ec0a2d9-6c10-40fa-bf0b-13b5d960f8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: dummy\n",
            "Hidden Dim: 256\n",
            "Batch Size: 2\n",
            "Epochs: 1\n",
            "Dummy Mode: True\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# å­¦ç¿’è¨­å®š - å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "# =============================================================================\n",
        "\n",
        "# å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: \"dummy\" | \"light\" | \"standard\"\n",
        "TRAIN_MODE = \"dummy\"\n",
        "\n",
        "# è¢«é¨“è€…ID\n",
        "SUBJECT = \"subj01\"\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆGoogle Driveä¸Šï¼‰\n",
        "DATA_ROOT = \"/content/data/algonauts_2023_challenge_data/train_data/\"\n",
        "\n",
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å…ˆ\n",
        "CHECKPOINT_DIR = \"/content/data/mindeye_checkpoints\"\n",
        "\n",
        "# æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»¢ç§»å­¦ç¿’ç”¨ã€Noneã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "PRETRAINED_CKPT = None  # ä¾‹: \"/content/drive/MyDrive/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
        "\n",
        "# =============================================================================\n",
        "# ãƒ¢ãƒ¼ãƒ‰åˆ¥è¨­å®šï¼ˆè‡ªå‹•è¨­å®šï¼‰\n",
        "# =============================================================================\n",
        "if TRAIN_MODE == \"dummy\":\n",
        "    HIDDEN_DIM = 256\n",
        "    BATCH_SIZE = 2\n",
        "    NUM_EPOCHS = 1\n",
        "    USE_PRIOR = False\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = True\n",
        "elif TRAIN_MODE == \"light\":\n",
        "    HIDDEN_DIM = 512\n",
        "    BATCH_SIZE = 4\n",
        "    NUM_EPOCHS = 10\n",
        "    USE_PRIOR = False\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = False\n",
        "else:  # standard\n",
        "    HIDDEN_DIM = 1024\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_EPOCHS = 50\n",
        "    USE_PRIOR = True\n",
        "    BLURRY_RECON = False\n",
        "    DUMMY_MODE = False\n",
        "\n",
        "print(f\"Mode: {TRAIN_MODE}\")\n",
        "print(f\"Hidden Dim: {HIDDEN_DIM}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Dummy Mode: {DUMMY_MODE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f892613",
      "metadata": {
        "id": "9f892613"
      },
      "source": [
        "## 2. ç’°å¢ƒæ§‹ç¯‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fef90d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fef90d8a",
        "outputId": "e0cfa08c-7e0b-4ef2-a095-e68344b66d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 13 02:53:36 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPUã®ç¢ºèª\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "24a979c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24a979c8",
        "outputId": "ae3b65f6-e6d2-4dfd-c4a1-04324250fc8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆå¸¸ã«å¿…è¦ï¼‰\n",
        "!pip install -q torch torchvision tqdm einops h5py pillow matplotlib requests\n",
        "!pip install -q omegaconf accelerate\n",
        "\n",
        "# OpenAI CLIPï¼ˆsrc/models.pyãŒå¿…è¦ï¼‰\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# webdatasetï¼ˆsrc/utils.pyãŒå¿…è¦ï¼‰\n",
        "!pip install -q webdataset\n",
        "\n",
        "# OpenCLIP\n",
        "!pip install -q open_clip_torch\n",
        "\n",
        "# ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ä¸€ã¤ä¸€ã¤è¿½åŠ ã—ã¦ã„ãã¾ã—ãŸã€‚\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q kornia\n",
        "!pip install -q dalle2-pytorch\n",
        "# DUMMY_MODE=Falseæ™‚ã®ã¿\n",
        "if not DUMMY_MODE:\n",
        "    !pip install -q diffusers transformers dalle2-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1062b8c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1062b8c4",
        "outputId": "6a58aa21-07a8-47fc-c033-54587a98ca37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already exists\n",
            "Working directory: /content/MindEyeV2\n"
          ]
        }
      ],
      "source": [
        "# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆæ—¢å­˜ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "import os\n",
        "if not os.path.exists(\"/content/MindEyeV2\"):\n",
        "    # TODO: è‡ªåˆ†ã®ãƒ•ã‚©ãƒ¼ã‚¯ã—ãŸãƒªãƒã‚¸ãƒˆãƒªURLã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "    !git clone https://github.com/boxed-mikann/MindEyeV2.git /content/MindEyeV2\n",
        "else:\n",
        "    print(\"Repository already exists\")\n",
        "\n",
        "# ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
        "import sys\n",
        "sys.path.insert(0, \"/content/MindEyeV2/mysrc\")\n",
        "sys.path.insert(0, \"/content/MindEyeV2/src\")\n",
        "\n",
        "os.chdir(\"/content/MindEyeV2\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pulling latest changes from MindEyeV2 repository...')\n",
        "!git pull\n",
        "print('Repository update complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StGS0JkILkyW",
        "outputId": "44c078d6-29f5-417c-f0cc-4d27f0134af3"
      },
      "id": "StGS0JkILkyW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling latest changes from MindEyeV2 repository...\n",
            "Already up to date.\n",
            "Repository update complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6985daca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6985daca",
        "outputId": "6960228f-386c-49f8-dbe2-a2eda2ac5a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Data found: /content/data/algonauts_2023_challenge_data/train_data/subj01\n",
            "total 20\n",
            "drwxr-xr-x 5 root root 4096 Jan 13 02:13 .\n",
            "drwxr-xr-x 3 root root 4096 Jan 13 02:13 ..\n",
            "drwxr-xr-x 2 root root 4096 Jan 13 02:13 roi_masks\n",
            "drwxr-xr-x 3 root root 4096 Jan 13 02:13 test_split\n",
            "drwxr-xr-x 4 root root 4096 Jan 13 02:14 training_split\n"
          ]
        }
      ],
      "source": [
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª\n",
        "import os\n",
        "subj_dir = os.path.join(DATA_ROOT, SUBJECT)\n",
        "if os.path.exists(subj_dir):\n",
        "    print(f\"âœ“ Data found: {subj_dir}\")\n",
        "    !ls -la {subj_dir}\n",
        "else:\n",
        "    print(f\"âœ— Data NOT found: {subj_dir}\")\n",
        "    print(\"Please upload Algonauts2023 data to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf51bdb",
      "metadata": {
        "id": "2bf51bdb"
      },
      "source": [
        "## 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f3bafa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f3bafa2",
        "outputId": "2c434f5e-7a99-4f29-bea9-321dc276c068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MindEyeV2 Algonauts Configuration\n",
            "============================================================\n",
            "Environment:     Colab\n",
            "Device:          cuda\n",
            "Dummy Mode:      True\n",
            "Data Root:       /content/drive/MyDrive/algonauts_2023_challenge_data\n",
            "Checkpoint Dir:  /content/drive/MyDrive/mindeye_checkpoints\n",
            "Output Dir:      /content/outputs\n",
            "SRC Dir:         /content/MindEyeV2/src\n",
            "============================================================\n",
            "\n",
            "Device: cuda\n",
            "Total vertices for subj01: 39548\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ç’°å¢ƒå¤‰æ•°ã§ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
        "os.environ[\"MINDEYE_DUMMY\"] = \"1\" if DUMMY_MODE else \"0\"\n",
        "\n",
        "# mysrcãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from algonauts_dataset import AlgonautsDataset, get_dataloader, get_total_vertices\n",
        "from config import print_config, DEVICE\n",
        "\n",
        "print_config()\n",
        "print(f\"\\nDevice: {DEVICE}\")\n",
        "print(f\"Total vertices for {SUBJECT}: {get_total_vertices(SUBJECT)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "936f9ce3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "936f9ce3",
        "outputId": "9d07f2d6-85c3-4d1f-a0c4-e38c2c74d15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fMRI data for subj01...\n",
            "  LH shape: (9841, 19004), RH shape: (9841, 20544)\n",
            "  Combined shape: (9841, 39548)\n",
            "Found 9841 train images\n",
            "\n",
            "Dataset size: 9841\n",
            "Number of batches: 4920\n",
            "\n",
            "Sample batch:\n",
            "  fMRI shape: torch.Size([2, 39548])\n",
            "  Image shape: torch.Size([2, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\n",
        "train_loader = get_dataloader(\n",
        "    data_root=DATA_ROOT,\n",
        "    subject=SUBJECT,\n",
        "    split=\"train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset size: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of batches: {len(train_loader)}\")\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒã‚’å–å¾—\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch:\")\n",
        "print(f\"  fMRI shape: {sample_batch['fmri'].shape}\")\n",
        "print(f\"  Image shape: {sample_batch['image'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3882cc",
      "metadata": {
        "id": "6e3882cc"
      },
      "source": [
        "## 4. ãƒ¢ãƒ‡ãƒ«ä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0f9d2104",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f9d2104",
        "outputId": "aa5071ea-0642-4edf-ec4e-d681ce5e092a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:generative_models.sgm.modules.diffusionmodules.model:no module 'xformers'. Processing without...\n",
            "WARNING:generative_models.sgm.modules.attention:no module 'xformers'. Processing without...\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created!\n",
            "============================================================\n",
            "Parameter Summary\n",
            "============================================================\n",
            "Total:      128,452,504\n",
            "Trainable:  128,452,504\n",
            "Frozen:               0\n",
            "------------------------------------------------------------\n",
            "By Layer:\n",
            "  ğŸŸ¢ ridge                10,124,544 (10,124,544 trainable)\n",
            "  ğŸŸ¢ backbone             118,327,960 (118,327,960 trainable)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from models_algonauts import AlgonautsMindEye, create_algonauts_model\n",
        "from transfer_utils import (\n",
        "    load_pretrained_without_ridge,\n",
        "    freeze_layers,\n",
        "    get_trainable_params,\n",
        "    print_parameter_summary,\n",
        ")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
        "model = create_algonauts_model(\n",
        "    subjects=[SUBJECT],\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    seq_len=1,\n",
        "    n_blocks=4,\n",
        "    use_prior=USE_PRIOR,\n",
        "    blurry_recon=BLURRY_RECON,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "print(\"Model created!\")\n",
        "print_parameter_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cb1f86db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb1f86db",
        "outputId": "7b6ed140-78fc-43c3-bfa8-bf2463ae8fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No pretrained checkpoint specified. Training from scratch.\n"
          ]
        }
      ],
      "source": [
        "# è»¢ç§»å­¦ç¿’ï¼ˆæ—¢å­˜ckptãŒã‚ã‚‹å ´åˆï¼‰\n",
        "if PRETRAINED_CKPT and os.path.exists(PRETRAINED_CKPT):\n",
        "    print(f\"Loading pretrained weights from: {PRETRAINED_CKPT}\")\n",
        "    loaded, missing = load_pretrained_without_ridge(\n",
        "        model,\n",
        "        PRETRAINED_CKPT,\n",
        "        freeze_backbone=True,\n",
        "        freeze_prior=True,\n",
        "    )\n",
        "    print(f\"\\nAfter transfer learning:\")\n",
        "    print_parameter_summary(model)\n",
        "else:\n",
        "    print(\"No pretrained checkpoint specified. Training from scratch.\")\n",
        "    # ã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’ã®å ´åˆã¯backboneã‚‚freezeã—ãªã„\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3d8f04",
      "metadata": {
        "id": "7b3d8f04"
      },
      "source": [
        "## 5. CLIPç‰¹å¾´ã®æº–å‚™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b0f201",
      "metadata": {
        "id": "f0b0f201"
      },
      "outputs": [],
      "source": [
        "# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼CLIPã‚’ä½¿ç”¨\n",
        "if DUMMY_MODE:\n",
        "    from dummy_models import DummyCLIPImageEmbedder, get_dummy_clip_features\n",
        "\n",
        "    clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)\n",
        "    print(\"Using DummyCLIPImageEmbedder\")\n",
        "else:\n",
        "    # æœ¬ç‰©ã®CLIPã‚’ä½¿ç”¨\n",
        "    try:\n",
        "        import open_clip\n",
        "\n",
        "        # ViT-bigG-14 ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆé‡ã„ï¼‰\n",
        "        print(\"Loading OpenCLIP ViT-bigG-14... (this may take a while)\")\n",
        "        clip_model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "            \"ViT-bigG-14\",\n",
        "            pretrained=\"laion2b_s39b_b160k\",\n",
        "        )\n",
        "        clip_model = clip_model.to(DEVICE).eval()\n",
        "\n",
        "        for param in clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(\"OpenCLIP loaded!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load OpenCLIP: {e}\")\n",
        "        print(\"Falling back to dummy mode\")\n",
        "        DUMMY_MODE = True\n",
        "        from dummy_models import DummyCLIPImageEmbedder\n",
        "        clip_embedder = DummyCLIPImageEmbedder().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412dc816",
      "metadata": {
        "id": "412dc816"
      },
      "outputs": [],
      "source": [
        "def get_clip_features(images):\n",
        "    \"\"\"ç”»åƒã‹ã‚‰CLIPç‰¹å¾´ã‚’æŠ½å‡º\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if DUMMY_MODE:\n",
        "            return clip_embedder(images)\n",
        "        else:\n",
        "            # OpenCLIPã‚’ä½¿ç”¨\n",
        "            features = clip_model.encode_image(images)\n",
        "            return features\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆ\n",
        "test_images = sample_batch['image'].to(DEVICE)\n",
        "test_features = get_clip_features(test_images)\n",
        "print(f\"CLIP features shape: {test_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e701814",
      "metadata": {
        "id": "8e701814"
      },
      "source": [
        "## 6. å­¦ç¿’ãƒ«ãƒ¼ãƒ—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d53128",
      "metadata": {
        "id": "a6d53128"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ï¼ˆå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ï¼‰\n",
        "trainable_params = get_trainable_params(model, mode=\"all_unfrozen\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "\n",
        "optimizer = AdamW(trainable_params, lr=3e-4, weight_decay=1e-2)\n",
        "\n",
        "# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=3e-4,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d06b396",
      "metadata": {
        "id": "0d06b396"
      },
      "outputs": [],
      "source": [
        "def soft_clip_loss(preds, targets, temp=0.006):\n",
        "    \"\"\"Soft CLIP contrastive loss\"\"\"\n",
        "    # Flatten to (batch, dim)\n",
        "    preds = preds.view(preds.shape[0], -1)\n",
        "    targets = targets.view(targets.shape[0], -1)\n",
        "\n",
        "    # Normalize\n",
        "    preds = F.normalize(preds, dim=-1)\n",
        "    targets = F.normalize(targets, dim=-1)\n",
        "\n",
        "    # Cosine similarity\n",
        "    logits = (preds @ targets.T) / temp\n",
        "    labels = torch.arange(len(logits), device=logits.device)\n",
        "\n",
        "    loss_i = F.cross_entropy(logits, labels)\n",
        "    loss_t = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    return (loss_i + loss_t) / 2\n",
        "\n",
        "def train_step(batch):\n",
        "    \"\"\"1ãƒãƒƒãƒã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "    fmri = batch['fmri'].to(DEVICE)\n",
        "    images = batch['image'].to(DEVICE)\n",
        "\n",
        "    # CLIPç‰¹å¾´ã‚’å–å¾—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰\n",
        "    with torch.no_grad():\n",
        "        clip_target = get_clip_features(images)\n",
        "\n",
        "    # Forward\n",
        "    backbone, clip_voxels, blurry = model(fmri)\n",
        "\n",
        "    # Lossè¨ˆç®—\n",
        "    # clip_voxels: (batch, seq, emb_dim)\n",
        "    # clip_target: (batch, seq, emb_dim) or (batch, emb_dim)\n",
        "    if clip_target.dim() == 2:\n",
        "        clip_target = clip_target.unsqueeze(1)\n",
        "\n",
        "    loss = soft_clip_loss(clip_voxels, clip_target)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d188f2f",
      "metadata": {
        "id": "0d188f2f"
      },
      "outputs": [],
      "source": [
        "# ãƒ¡ãƒ¢ãƒªç¢ºèª\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e0367f",
      "metadata": {
        "id": "51e0367f"
      },
      "outputs": [],
      "source": [
        "# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting training: {NUM_EPOCHS} epochs, {len(train_loader)} batches/epoch\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_losses = []\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    for batch in pbar:\n",
        "        loss = train_step(batch)\n",
        "        epoch_losses.append(loss)\n",
        "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
        "\n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    losses.extend(epoch_losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # ãƒ¡ãƒ¢ãƒªç¢ºèª\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training complete!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cf52af",
      "metadata": {
        "id": "f1cf52af"
      },
      "outputs": [],
      "source": [
        "# æå¤±ã®å¯è¦–åŒ–\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70e4d2f",
      "metadata": {
        "id": "f70e4d2f"
      },
      "source": [
        "## 7. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b93a67",
      "metadata": {
        "id": "39b93a67"
      },
      "outputs": [],
      "source": [
        "from transfer_utils import save_checkpoint\n",
        "\n",
        "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
        "save_dir = os.path.join(CHECKPOINT_DIR, f\"algonauts_{SUBJECT}_{TRAIN_MODE}\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
        "save_checkpoint(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    epoch=NUM_EPOCHS,\n",
        "    save_path=os.path.join(save_dir, \"last.pth\"),\n",
        "    extra_info={\n",
        "        \"train_mode\": TRAIN_MODE,\n",
        "        \"subject\": SUBJECT,\n",
        "        \"hidden_dim\": HIDDEN_DIM,\n",
        "        \"final_loss\": losses[-1] if losses else None,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"\\nCheckpoint saved to: {save_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c46259",
      "metadata": {
        "id": "d2c46259"
      },
      "source": [
        "## 8. ç°¡æ˜“æ¤œè¨¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38dee4e2",
      "metadata": {
        "id": "38dee4e2"
      },
      "outputs": [],
      "source": [
        "# æ¨è«–ãƒ†ã‚¹ãƒˆ\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_batch = next(iter(train_loader))\n",
        "    test_fmri = test_batch['fmri'].to(DEVICE)\n",
        "    test_images = test_batch['image'].to(DEVICE)\n",
        "\n",
        "    # fMRI â†’ CLIP tokens\n",
        "    backbone, clip_voxels, blurry = model(test_fmri)\n",
        "\n",
        "    # å®Ÿéš›ã®CLIPç‰¹å¾´\n",
        "    clip_target = get_clip_features(test_images)\n",
        "    if clip_target.dim() == 2:\n",
        "        clip_target = clip_target.unsqueeze(1)\n",
        "\n",
        "    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
        "    pred_flat = F.normalize(clip_voxels.view(clip_voxels.shape[0], -1), dim=-1)\n",
        "    target_flat = F.normalize(clip_target.view(clip_target.shape[0], -1), dim=-1)\n",
        "\n",
        "    similarity = (pred_flat * target_flat).sum(dim=-1).mean()\n",
        "\n",
        "    print(f\"\\nInference test:\")\n",
        "    print(f\"  Input fMRI shape: {test_fmri.shape}\")\n",
        "    print(f\"  Output CLIP shape: {clip_voxels.shape}\")\n",
        "    print(f\"  Average cosine similarity: {similarity.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d962d6be",
      "metadata": {
        "id": "d962d6be"
      },
      "outputs": [],
      "source": [
        "# å…¥åŠ›ç”»åƒã®å¯è¦–åŒ–\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"ImageNetæ­£è¦åŒ–ã‚’å…ƒã«æˆ»ã™\"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
        "    return tensor * std + mean\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\n",
        "sample_images = denormalize(test_images[:4])\n",
        "grid = make_grid(sample_images, nrow=4).cpu().permute(1, 2, 0).numpy()\n",
        "grid = np.clip(grid, 0, 1)\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.imshow(grid)\n",
        "plt.title(\"Sample Training Images\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d5bbd3",
      "metadata": {
        "id": "f9d5bbd3"
      },
      "source": [
        "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "\n",
        "1. **ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œç¢ºèª** â†’ ã‚¨ãƒ©ãƒ¼ãªãå®Œäº†ã™ã‚Œã°OK\n",
        "2. **è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆlightï¼‰ã§å®Ÿå­¦ç¿’** â†’ T4ã§æ•°æ™‚é–“\n",
        "3. **æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼ˆstandardï¼‰ã§æœ¬æ ¼å­¦ç¿’** â†’ Pro or ç ”ç©¶å®¤PC\n",
        "4. **æ¨è«–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯** â†’ `mindeye_inference_colab.ipynb` ã§ç”»åƒå†æ§‹æˆ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}